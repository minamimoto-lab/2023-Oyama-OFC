# -*- coding: utf-8 -*-
"""RLmodelForReversalTask_20210901.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XzrS_oIA3C5bOj9T55XRfwsVdBWIWx3B
"""

#On 20201229, BIC and AIC computation were added.
#On 20201231, the Bayesian model comparison based on "Bayesian model selection for group studies. Neuroimage."ã€€ in Neuroimage 2009.
#On 20210108 (& 20210109), RL models with two leaning rate (alpha) parameters were implemented, which may explain the difference between new-leaning and re-learning.
#On 20210110, to avoid local minima in the estimation of alpha and beta, modify the estimation procedure so that it repeats optimization (asipy.optimize.minimize) 50 times with different initial values and adopts the best ones.
#On 20210221, RL models with a temporally changing learning rate was introduced.
#On 20210405, the optimization process was updated (and improved).
#1) Parameter estimation with parameters out of the range is avoided.
#2) Regularization to prevent parameters from taking very very large values by accident.
#3) Two step-optimization was introduced.
#On 20210407_old, Nelder-Mead and Powell were compared. The results indicate that Powell is better.
#On 20210407, based on the above results, Powell optimization method was adopted. Other minor points were also modified to improve the stability.
#On 20210524, the code was modified so that it would test both Powell and Nelder-Mead and finally adopt the better one in terms of AIC since I've found that the best method varies across datasets.
#On 20210607, regularization coefficients are set to 10.0 (larger compared with the previous versions) to avoid unsuitable estimation.
#On 20210718, regularization coefficient are set to 1.0, which is the same as 20210524. Then two new models are added.
#1) models with different beta values corresponding trial groups. 2) model with adaptive learnin rate (see Li et al., nature neuroscience, 2011[Differential roles of human striatum and amygdala in associative learning])
#On 20210901, the functions for simulation (e.g., simulate_modelBased_temporallyChangingAlpha) were updated so that they can also treat time-dependent beta.
import numpy as np
import itertools
import matplotlib.pyplot as plt
import scipy.optimize
import scipy.special
import random
import sys
import pandas as pd
import os

#define basic functions used in this code.
def softmax(actionValue, beta):
  x = actionValue*beta
  e_x = np.exp(x - np.max(x))
  output = e_x / e_x.sum()
  # to avoid log(0) computation, the minimum value was introduced.
  np.place(output, output < 10**(-200) , 10**(-200))
  return output

def make1blockStimSequence():
  stimuli = ['S1','S2','S3','S4','S5']
  stimSeq = list(itertools.combinations(stimuli, 2))
  random.shuffle(stimSeq)
  return stimSeq

def returnReward(choice, rewardRulePattern):
  reward = 0
  if rewardRulePattern == 'A':
    if choice == 'S1':
      reward = 1
    elif choice == 'S2':
      reward = 2
    elif choice == 'S3':
      reward = 3
    elif choice == 'S4':
      reward = 4
    elif choice == 'S5':
      reward = 5
    else:
      print('Error: The chosen item must be S1, S2, S3, S4, or S5.')
      sys.exit(1)
  elif rewardRulePattern == 'B':
    if choice == 'S1':
      reward = 5
    elif choice == 'S2':
      reward = 4
    elif choice == 'S3':
      reward = 3
    elif choice == 'S4':
      reward = 2
    elif choice == 'S5':
      reward = 1
    else:
      print('Error: The chosen item must be S1, S2, S3, S4, or S5.')
      sys.exit(1) 
  else:
    print('Error: Reward rule pattern must be A or B.')
    sys.exit(1) 
  return reward

def optimalChoiceRate(stimSeq, rewardRulePattern,choiceSeq):
  stimuli = ['S1','S2','S3','S4','S5']
  optimalOrNot = np.zeros(len(stimSeq))
  for index_trial in range(len(optimalOrNot)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    currentChoice = stimuli.index(choiceSeq[index_trial])
    if rewardRulePattern[index_trial]=='A':
      if currentChoice==np.max([stimLeft,stimRight]):
        optimalOrNot[index_trial] = 1
    elif rewardRulePattern[index_trial]=='B':
      if currentChoice==np.min([stimLeft,stimRight]):
        optimalOrNot[index_trial] = 1
    else:
      print('Error: Reward rule pattern must be A or B.')
      sys.exit(1)
  rate = np.zeros(round(len(stimSeq)/10))
  for index_block in range(round(len(stimSeq)/10)):
    rate[index_block] = np.mean(optimalOrNot[(index_block*10):((index_block+1)*10)])
  return rate

def simulate_modelFree(stimSeq,rewardRulePattern,alpha,beta):
  actionValue = np.zeros(5)
  stimuli = ['S1','S2','S3','S4','S5']
  choiceSeq = list()
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = softmax(np.array([actionValue[stimLeft],actionValue[stimRight]]), beta)
    currentChoice = np.random.choice([stimuli[stimLeft], stimuli[stimRight]], size=(1,), p=prob)
    currentChoice = currentChoice[0]
    choiceSeq.append(currentChoice)
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - actionValue[stimuli.index(currentChoice)]
    actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha*rewardPredictionError
  return choiceSeq

#Added on 20210108.
#In this model, two learning rate parameters are used.
def simulate_modelFree_asymmetric(stimSeq,rewardRulePattern,alpha_plus,alpha_minus,beta):
  actionValue = np.zeros(5)
  stimuli = ['S1','S2','S3','S4','S5']
  choiceSeq = list()
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = softmax(np.array([actionValue[stimLeft],actionValue[stimRight]]), beta)
    currentChoice = np.random.choice([stimuli[stimLeft], stimuli[stimRight]], size=(1,), p=prob)
    currentChoice = currentChoice[0]
    choiceSeq.append(currentChoice)
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - actionValue[stimuli.index(currentChoice)]
    if rewardPredictionError > 0:
      actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha_plus*rewardPredictionError
    else:
      actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha_minus*rewardPredictionError
  return choiceSeq

#Added on 20210221.
#In this model, trials are grouped by the user and alpha is estimated for each trial group.
#For example, if you group trials 0-69 trials, 70-89 trials, 90-159 trials, & 159-299 trials into the four groups (four periods).
#Four alpha parameters are estimated, which are cprresponding to 'learning phase', 'stable phase 1', 're-learning phase' and 'stable phase 2'.
def simulate_modelFree_temporallyChangingAlpha(stimSeq,rewardRulePattern,alpha,beta,trialGroup):
  actionValue = np.zeros(5)
  stimuli = ['S1','S2','S3','S4','S5']
  choiceSeq = list()
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = softmax(np.array([actionValue[stimLeft],actionValue[stimRight]]), beta)
    currentChoice = np.random.choice([stimuli[stimLeft], stimuli[stimRight]], size=(1,), p=prob)
    currentChoice = currentChoice[0]
    choiceSeq.append(currentChoice)
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - actionValue[stimuli.index(currentChoice)]
    #choose used alpha depending on the trial group.
    currentTrialGroup = -1
    for index_trialGroup in range(len(trialGroup)):
      if np.any(trialGroup[index_trialGroup]==index_trial):
        if currentTrialGroup == -1:
          currentTrialGroup = index_trialGroup
        else:
          print('Error: the same trial is used in multiple groups you specified.')
          sys.exit(1)
    if currentTrialGroup == -1:
      print('Error: trial # ' + str(index_trial) + ' does not belong to any group you specified.')
      sys.exit(1)
    actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha[currentTrialGroup]*rewardPredictionError
  return choiceSeq

#Added on 20210901.
#Based on the above function, not only alpha, but also beta is estimated in each trial group.
def simulate_modelFree_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,alpha,beta,trialGroup):
  actionValue = np.zeros(5)
  stimuli = ['S1','S2','S3','S4','S5']
  choiceSeq = list()
  for index_trial in range(len(stimSeq)):
    #choose used alpha and beta depending on the trial group.
    currentTrialGroup = -1
    for index_trialGroup in range(len(trialGroup)):
      if np.any(trialGroup[index_trialGroup]==index_trial):
        if currentTrialGroup == -1:
          currentTrialGroup = index_trialGroup
        else:
          print('Error: the same trial is used in multiple groups you specified.')
          sys.exit(1)
    if currentTrialGroup == -1:
      print('Error: trial # ' + str(index_trial) + ' does not belong to any group you specified.')
      sys.exit(1)
    #Choose the action
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = softmax(np.array([actionValue[stimLeft],actionValue[stimRight]]), beta[currentTrialGroup])
    currentChoice = np.random.choice([stimuli[stimLeft], stimuli[stimRight]], size=(1,), p=prob)
    currentChoice = currentChoice[0]
    choiceSeq.append(currentChoice)
    #Update the value functions based on the reward.
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - actionValue[stimuli.index(currentChoice)]
    actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha[currentTrialGroup]*rewardPredictionError
  return choiceSeq

def simulate_modelBased(stimSeq,rewardRulePattern,alpha,beta):
  modelRatio = 0.5
  stimuli = ['S1','S2','S3','S4','S5']
  choiceSeq = list()
  actionValue4patternA = np.array([1.0,2.0,3.0,4.0,5.0]) 
  actionValue4patternB = np.array([5.0,4.0,3.0,2.0,1.0]) 
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = modelRatio*softmax(np.array([actionValue4patternA[stimLeft],actionValue4patternA[stimRight]]), beta)+(1.0-modelRatio)*softmax(np.array([actionValue4patternB[stimLeft],actionValue4patternB[stimRight]]), beta)
    currentChoice = np.random.choice([stimuli[stimLeft], stimuli[stimRight]], size=(1,), p=prob)
    currentChoice = currentChoice[0]
    choiceSeq.append(currentChoice)
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - modelRatio*actionValue4patternA[stimuli.index(currentChoice)] - (1.0-modelRatio)*actionValue4patternB[stimuli.index(currentChoice)]
    modelRatio = modelRatio + alpha*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    if modelRatio > 1.0:
      modelRatio = 1.0
    elif modelRatio < 0.0:
      modelRatio = 0.0
  return choiceSeq

def simulate_modelBased_asymmetric(stimSeq,rewardRulePattern,alpha_plus,alpha_minus,beta):
  modelRatio = 0.5
  stimuli = ['S1','S2','S3','S4','S5']
  choiceSeq = list()
  actionValue4patternA = np.array([1.0,2.0,3.0,4.0,5.0]) 
  actionValue4patternB = np.array([5.0,4.0,3.0,2.0,1.0]) 
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = modelRatio*softmax(np.array([actionValue4patternA[stimLeft],actionValue4patternA[stimRight]]), beta)+(1.0-modelRatio)*softmax(np.array([actionValue4patternB[stimLeft],actionValue4patternB[stimRight]]), beta)
    currentChoice = np.random.choice([stimuli[stimLeft], stimuli[stimRight]], size=(1,), p=prob)
    currentChoice = currentChoice[0]
    choiceSeq.append(currentChoice)
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - modelRatio*actionValue4patternA[stimuli.index(currentChoice)] - (1.0-modelRatio)*actionValue4patternB[stimuli.index(currentChoice)]
    if rewardPredictionError > 0:
      modelRatio = modelRatio + alpha_plus*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    else:
      modelRatio = modelRatio + alpha_minus*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    if modelRatio > 1.0:
      modelRatio = 1.0
    elif modelRatio < 0.0:
      modelRatio = 0.0
  return choiceSeq

#Added on 20210221.
#In this model, trials are grouped by the user and alpha is estimated for each trial group.
#For example, if you group trials 0-69 trials, 70-89 trials, 90-159 trials, & 159-299 trials into the four groups (four periods).
#Four alpha parameters are estimated, which are cprresponding to 'learning phase', 'stable phase 1', 're-learning phase' and 'stable phase 2'.
def simulate_modelBased_temporallyChangingAlpha(stimSeq,rewardRulePattern,alpha,beta,trialGroup):
  modelRatio = 0.5
  stimuli = ['S1','S2','S3','S4','S5']
  choiceSeq = list()
  actionValue4patternA = np.array([1.0,2.0,3.0,4.0,5.0]) 
  actionValue4patternB = np.array([5.0,4.0,3.0,2.0,1.0]) 
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = modelRatio*softmax(np.array([actionValue4patternA[stimLeft],actionValue4patternA[stimRight]]), beta)+(1.0-modelRatio)*softmax(np.array([actionValue4patternB[stimLeft],actionValue4patternB[stimRight]]), beta)
    currentChoice = np.random.choice([stimuli[stimLeft], stimuli[stimRight]], size=(1,), p=prob)
    currentChoice = currentChoice[0]
    choiceSeq.append(currentChoice)
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - modelRatio*actionValue4patternA[stimuli.index(currentChoice)] - (1.0-modelRatio)*actionValue4patternB[stimuli.index(currentChoice)]
    #choose used alpha depending on the trial group.
    currentTrialGroup = -1
    for index_trialGroup in range(len(trialGroup)):
      if np.any(trialGroup[index_trialGroup]==index_trial):
        if currentTrialGroup == -1:
          currentTrialGroup = index_trialGroup
        else:
          print('Error: the same trial is used in multiple groups you specified.')
          sys.exit(1)
    if currentTrialGroup == -1:
      print('Error: trial # ' + str(index_trial) + ' does not belong to any group you specified.')
      sys.exit(1)
    modelRatio = modelRatio + alpha[currentTrialGroup]*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    if modelRatio > 1.0:
      modelRatio = 1.0
    elif modelRatio < 0.0:
      modelRatio = 0.0
  return choiceSeq

#Added on 20210901.
#Based on the above function, not only alpha, but also beta is estimated in each trial group.
def simulate_modelBased_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,alpha,beta,trialGroup):
  modelRatio = 0.5
  stimuli = ['S1','S2','S3','S4','S5']
  choiceSeq = list()
  actionValue4patternA = np.array([1.0,2.0,3.0,4.0,5.0]) 
  actionValue4patternB = np.array([5.0,4.0,3.0,2.0,1.0]) 
  for index_trial in range(len(stimSeq)):
    #choose used alpha and beta depending on the trial group.
    currentTrialGroup = -1
    for index_trialGroup in range(len(trialGroup)):
      if np.any(trialGroup[index_trialGroup]==index_trial):
        if currentTrialGroup == -1:
          currentTrialGroup = index_trialGroup
        else:
          print('Error: the same trial is used in multiple groups you specified.')
          sys.exit(1)
    if currentTrialGroup == -1:
      print('Error: trial # ' + str(index_trial) + ' does not belong to any group you specified.')
      sys.exit(1)
    #Choose the action
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = modelRatio*softmax(np.array([actionValue4patternA[stimLeft],actionValue4patternA[stimRight]]), beta[currentTrialGroup])+(1.0-modelRatio)*softmax(np.array([actionValue4patternB[stimLeft],actionValue4patternB[stimRight]]), beta[currentTrialGroup])
    currentChoice = np.random.choice([stimuli[stimLeft], stimuli[stimRight]], size=(1,), p=prob)
    currentChoice = currentChoice[0]
    choiceSeq.append(currentChoice)
    #Update the value functions based on the reward.
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - modelRatio*actionValue4patternA[stimuli.index(currentChoice)] - (1.0-modelRatio)*actionValue4patternB[stimuli.index(currentChoice)]
    modelRatio = modelRatio + alpha[currentTrialGroup]*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    if modelRatio > 1.0:
      modelRatio = 1.0
    elif modelRatio < 0.0:
      modelRatio = 0.0
  return choiceSeq

def logLikelihood_modelFree(stimSeq,rewardRulePattern,choiceSeq,alpha,beta):
  actionValue = np.zeros(5)
  stimuli = ['S1','S2','S3','S4','S5']
  logLikelihood = 0
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = softmax(np.array([actionValue[stimLeft],actionValue[stimRight]]), beta)
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - actionValue[stimuli.index(currentChoice)]
    actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha*rewardPredictionError
  if (alpha < 0) or (beta < 0):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + 2.0*2.0
  BIC = -2.0*logLikelihood + 2.0*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC

def logLikelihood_modelFree_asymmetric(stimSeq,rewardRulePattern,choiceSeq,alpha_plus,alpha_minus,beta):
  actionValue = np.zeros(5)
  stimuli = ['S1','S2','S3','S4','S5']
  logLikelihood = 0
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = softmax(np.array([actionValue[stimLeft],actionValue[stimRight]]), beta)
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - actionValue[stimuli.index(currentChoice)]
    if rewardPredictionError > 0:
      actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha_plus*rewardPredictionError
    else:
      actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha_minus*rewardPredictionError
  if ((alpha_plus < 0) or (alpha_minus < 0)) or (beta < 0):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + 3.0*2.0
  BIC = -2.0*logLikelihood + 3.0*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC

def logLikelihood_modelFree_temporallyChangingAlpha(stimSeq,rewardRulePattern,choiceSeq,alpha,beta,trialGroup):
  actionValue = np.zeros(5)
  stimuli = ['S1','S2','S3','S4','S5']
  logLikelihood = 0
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = softmax(np.array([actionValue[stimLeft],actionValue[stimRight]]), beta)
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - actionValue[stimuli.index(currentChoice)]
    #choose used alpha depending on the trial group.
    currentTrialGroup = -1
    for index_trialGroup in range(len(trialGroup)):
      if np.any(trialGroup[index_trialGroup]==index_trial):
        if currentTrialGroup == -1:
          currentTrialGroup = index_trialGroup
        else:
          print('Error: the same trial is used in multiple groups you specified.')
          sys.exit(1)
    if currentTrialGroup == -1:
      print('Error: trial # ' + str(index_trial) + ' does not belong to any group you specified.')
      sys.exit(1)
    actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha[currentTrialGroup]*rewardPredictionError
  if any(alpha < 0) or (beta < 0):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + (len(trialGroup)+1)*2.0
  BIC = -2.0*logLikelihood + (len(trialGroup)+1)*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC

#Added on 20210718.
def logLikelihood_modelFree_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,choiceSeq,alpha,beta,trialGroup):
  actionValue = np.zeros(5)
  stimuli = ['S1','S2','S3','S4','S5']
  logLikelihood = 0
  for index_trial in range(len(stimSeq)):
    #choose used alpha and beta depending on the trial group.
    currentTrialGroup = -1
    for index_trialGroup in range(len(trialGroup)):
      if np.any(trialGroup[index_trialGroup]==index_trial):
        if currentTrialGroup == -1:
          currentTrialGroup = index_trialGroup
        else:
          print('Error: the same trial is used in multiple groups you specified.')
          sys.exit(1)
    if currentTrialGroup == -1:
      print('Error: trial # ' + str(index_trial) + ' does not belong to any group you specified.')
      sys.exit(1)
    #Action choice
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = softmax(np.array([actionValue[stimLeft],actionValue[stimRight]]), beta[currentTrialGroup])
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - actionValue[stimuli.index(currentChoice)]
    actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha[currentTrialGroup]*rewardPredictionError
  if any(alpha < 0) or any(beta < 0):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + (2*len(trialGroup))*2.0
  BIC = -2.0*logLikelihood + (2*len(trialGroup))*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC

#Added on 20210718.
def logLikelihood_modelFree_adaptiveLR(stimSeq,rewardRulePattern,choiceSeq,kappa,beta,eta,alpha0):
  actionValue = np.zeros(5)
  stimuli = ['S1','S2','S3','S4','S5']
  logLikelihood = 0
  alpha = np.zeros(len(stimSeq))
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = softmax(np.array([actionValue[stimLeft],actionValue[stimRight]]), beta)
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    #Compute adaptive alpha using the reward prediction error in the last trial
    if index_trial == 0:
      alpha[index_trial]=alpha0*kappa
    else:
      alpha[index_trial]=kappa*(eta*abs(rewardPredictionError)+(1-eta))#(see Li et al., nature neuroscience, 2011))
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - actionValue[stimuli.index(currentChoice)]
    actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha[index_trial]*rewardPredictionError
  if (kappa < 0) or (beta < 0) or (eta < 0) or (eta > 1):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + 4.0*2.0
  BIC = -2.0*logLikelihood + 4.0*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC, alpha


def logLikelihood_modelFree_adaptiveLR2(stimSeq,rewardRulePattern,choiceSeq,kappa,beta,eta):
  #Almost the same as the above, but alpha0=0.
  actionValue = np.zeros(5)
  stimuli = ['S1','S2','S3','S4','S5']
  logLikelihood = 0
  alpha = np.zeros(len(stimSeq))
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = softmax(np.array([actionValue[stimLeft],actionValue[stimRight]]), beta)
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    #Compute adaptive alpha using the reward prediction error in the last trial
    if index_trial == 0:
      alpha[index_trial]=0
    else:
      alpha[index_trial]=kappa*(eta*abs(rewardPredictionError)+(1-eta))#(see Li et al., nature neuroscience, 2011))
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - actionValue[stimuli.index(currentChoice)]
    actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha[index_trial]*rewardPredictionError
  if (kappa < 0) or (beta < 0) or (eta < 0) or (eta > 1):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + 3.0*2.0
  BIC = -2.0*logLikelihood + 3.0*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC, alpha

def logLikelihood_modelBased(stimSeq,rewardRulePattern,choiceSeq,alpha,beta):
  modelRatio = 0.5
  stimuli = ['S1','S2','S3','S4','S5']
  actionValue4patternA = np.array([1.0,2.0,3.0,4.0,5.0]) 
  actionValue4patternB = np.array([5.0,4.0,3.0,2.0,1.0]) 
  logLikelihood = 0
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = modelRatio*softmax(np.array([actionValue4patternA[stimLeft],actionValue4patternA[stimRight]]), beta)+(1.0-modelRatio)*softmax(np.array([actionValue4patternB[stimLeft],actionValue4patternB[stimRight]]), beta)
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - modelRatio*actionValue4patternA[stimuli.index(currentChoice)] - (1.0-modelRatio)*actionValue4patternB[stimuli.index(currentChoice)]
    modelRatio = modelRatio + alpha*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    if modelRatio > 1.0:
      modelRatio = 1.0
    elif modelRatio < 0.0:
      modelRatio = 0.0
  if (alpha < 0) or (beta < 0):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + 2.0*2.0
  BIC = -2.0*logLikelihood + 2.0*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC

def logLikelihood_modelBased_asymmetric(stimSeq,rewardRulePattern,choiceSeq,alpha_plus,alpha_minus,beta):
  modelRatio = 0.5
  stimuli = ['S1','S2','S3','S4','S5']
  actionValue4patternA = np.array([1.0,2.0,3.0,4.0,5.0]) 
  actionValue4patternB = np.array([5.0,4.0,3.0,2.0,1.0]) 
  logLikelihood = 0
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = modelRatio*softmax(np.array([actionValue4patternA[stimLeft],actionValue4patternA[stimRight]]), beta)+(1.0-modelRatio)*softmax(np.array([actionValue4patternB[stimLeft],actionValue4patternB[stimRight]]), beta)
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - modelRatio*actionValue4patternA[stimuli.index(currentChoice)] - (1.0-modelRatio)*actionValue4patternB[stimuli.index(currentChoice)]
    if rewardPredictionError > 0:
      modelRatio = modelRatio + alpha_plus*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    else:
      modelRatio = modelRatio + alpha_minus*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    if modelRatio > 1.0:
      modelRatio = 1.0
    elif modelRatio < 0.0:
      modelRatio = 0.0
  if ((alpha_plus < 0) or (alpha_minus < 0)) or (beta < 0):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + 3.0*2.0
  BIC = -2.0*logLikelihood + 3.0*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC

def logLikelihood_modelBased_temporallyChangingAlpha(stimSeq,rewardRulePattern,choiceSeq,alpha,beta,trialGroup):
  modelRatio = 0.5
  stimuli = ['S1','S2','S3','S4','S5']
  actionValue4patternA = np.array([1.0,2.0,3.0,4.0,5.0]) 
  actionValue4patternB = np.array([5.0,4.0,3.0,2.0,1.0]) 
  logLikelihood = 0
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = modelRatio*softmax(np.array([actionValue4patternA[stimLeft],actionValue4patternA[stimRight]]), beta)+(1.0-modelRatio)*softmax(np.array([actionValue4patternB[stimLeft],actionValue4patternB[stimRight]]), beta)
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - modelRatio*actionValue4patternA[stimuli.index(currentChoice)] - (1.0-modelRatio)*actionValue4patternB[stimuli.index(currentChoice)]
    #choose used alpha depending on the trial group.
    currentTrialGroup = -1
    for index_trialGroup in range(len(trialGroup)):
      if np.any(trialGroup[index_trialGroup]==index_trial):
        if currentTrialGroup == -1:
          currentTrialGroup = index_trialGroup
        else:
          print('Error: the same trial is used in multiple groups you specified.')
          sys.exit(1)
    if currentTrialGroup == -1:
      print('Error: trial # ' + str(index_trial) + ' does not belong to any group you specified.')
      sys.exit(1)
    modelRatio = modelRatio + alpha[currentTrialGroup]*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    if modelRatio > 1.0:
      modelRatio = 1.0
    elif modelRatio < 0.0:
      modelRatio = 0.0
  if any(alpha < 0) or (beta < 0):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + (len(trialGroup)+1)*2.0
  BIC = -2.0*logLikelihood + (len(trialGroup)+1)*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC

#Added on 20210718.
def logLikelihood_modelBased_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,choiceSeq,alpha,beta,trialGroup):
  modelRatio = 0.5
  stimuli = ['S1','S2','S3','S4','S5']
  actionValue4patternA = np.array([1.0,2.0,3.0,4.0,5.0]) 
  actionValue4patternB = np.array([5.0,4.0,3.0,2.0,1.0]) 
  logLikelihood = 0
  for index_trial in range(len(stimSeq)):
    #choose used alpha and beta depending on the trial group.
    currentTrialGroup = -1
    for index_trialGroup in range(len(trialGroup)):
      if np.any(trialGroup[index_trialGroup]==index_trial):
        if currentTrialGroup == -1:
          currentTrialGroup = index_trialGroup
        else:
          print('Error: the same trial is used in multiple groups you specified.')
          sys.exit(1)
    if currentTrialGroup == -1:
      print('Error: trial # ' + str(index_trial) + ' does not belong to any group you specified.')
      sys.exit(1)
    #Action choice
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = modelRatio*softmax(np.array([actionValue4patternA[stimLeft],actionValue4patternA[stimRight]]), beta[currentTrialGroup])+(1.0-modelRatio)*softmax(np.array([actionValue4patternB[stimLeft],actionValue4patternB[stimRight]]), beta[currentTrialGroup])
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - modelRatio*actionValue4patternA[stimuli.index(currentChoice)] - (1.0-modelRatio)*actionValue4patternB[stimuli.index(currentChoice)]
    modelRatio = modelRatio + alpha[currentTrialGroup]*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    if modelRatio > 1.0:
      modelRatio = 1.0
    elif modelRatio < 0.0:
      modelRatio = 0.0
  if any(alpha < 0) or any(beta < 0):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + (2*len(trialGroup))*2.0
  BIC = -2.0*logLikelihood + (2*len(trialGroup))*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC

#Added on 20210718.
def logLikelihood_modelBased_adaptiveLR(stimSeq,rewardRulePattern,choiceSeq,kappa,beta,eta,alpha0):
  modelRatio = 0.5
  stimuli = ['S1','S2','S3','S4','S5']
  actionValue4patternA = np.array([1.0,2.0,3.0,4.0,5.0]) 
  actionValue4patternB = np.array([5.0,4.0,3.0,2.0,1.0]) 
  logLikelihood = 0
  alpha = np.zeros(len(stimSeq))
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = modelRatio*softmax(np.array([actionValue4patternA[stimLeft],actionValue4patternA[stimRight]]), beta)+(1.0-modelRatio)*softmax(np.array([actionValue4patternB[stimLeft],actionValue4patternB[stimRight]]), beta)
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    #Compute adaptive alpha using the reward prediction error in the last trial
    if index_trial == 0:
      alpha[index_trial]=alpha0*kappa
    else:
      alpha[index_trial]=kappa*(eta*abs(rewardPredictionError)+(1-eta))#(see Li et al., nature neuroscience, 2011))
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - modelRatio*actionValue4patternA[stimuli.index(currentChoice)] - (1.0-modelRatio)*actionValue4patternB[stimuli.index(currentChoice)]
    modelRatio = modelRatio + alpha[index_trial]*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    if modelRatio > 1.0:
      modelRatio = 1.0
    elif modelRatio < 0.0:
      modelRatio = 0.0
  if (kappa < 0) or (beta < 0) or (eta < 0) or (eta > 1):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + 4.0*2.0
  BIC = -2.0*logLikelihood + 4.0*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC, alpha

def logLikelihood_modelBased_adaptiveLR2(stimSeq,rewardRulePattern,choiceSeq,kappa,beta,eta):
  #Almost the same as the above, but alpha0=0.
  modelRatio = 0.5
  stimuli = ['S1','S2','S3','S4','S5']
  actionValue4patternA = np.array([1.0,2.0,3.0,4.0,5.0]) 
  actionValue4patternB = np.array([5.0,4.0,3.0,2.0,1.0]) 
  logLikelihood = 0
  alpha = np.zeros(len(stimSeq))
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = modelRatio*softmax(np.array([actionValue4patternA[stimLeft],actionValue4patternA[stimRight]]), beta)+(1.0-modelRatio)*softmax(np.array([actionValue4patternB[stimLeft],actionValue4patternB[stimRight]]), beta)
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    #Compute adaptive alpha using the reward prediction error in the last trial
    if index_trial == 0:
      alpha[index_trial]=0
    else:
      alpha[index_trial]=kappa*(eta*abs(rewardPredictionError)+(1-eta))#(see Li et al., nature neuroscience, 2011))
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - modelRatio*actionValue4patternA[stimuli.index(currentChoice)] - (1.0-modelRatio)*actionValue4patternB[stimuli.index(currentChoice)]
    modelRatio = modelRatio + alpha[index_trial]*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    if modelRatio > 1.0:
      modelRatio = 1.0
    elif modelRatio < 0.0:
      modelRatio = 0.0
  if (kappa < 0) or (beta < 0) or (eta < 0) or (eta > 1):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + 3.0*2.0
  BIC = -2.0*logLikelihood + 3.0*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC, alpha

def costfunc2minimize_modelFree(param,stimSeq,rewardRulePattern,choiceSeq):
  targetValue, AIC, BIC = logLikelihood_modelFree(stimSeq,rewardRulePattern,choiceSeq,param[0],param[1])
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue
def costfunc2minimize_modelBased(param,stimSeq,rewardRulePattern,choiceSeq):
  targetValue, AIC, BIC = logLikelihood_modelBased(stimSeq,rewardRulePattern,choiceSeq,param[0],param[1])
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue
def costfunc2minimize_modelFree_asymmetric(param,stimSeq,rewardRulePattern,choiceSeq):
  targetValue, AIC, BIC = logLikelihood_modelFree_asymmetric(stimSeq,rewardRulePattern,choiceSeq,param[0],param[1],param[2])
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue
def costfunc2minimize_modelBased_asymmetric(param,stimSeq,rewardRulePattern,choiceSeq):
  targetValue, AIC, BIC = logLikelihood_modelBased_asymmetric(stimSeq,rewardRulePattern,choiceSeq,param[0],param[1],param[2])
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue
def costfunc2minimize_modelFree_temporallyChangingAlpha(param,stimSeq,rewardRulePattern,choiceSeq,trialGroup):
  targetValue, AIC, BIC = logLikelihood_modelFree_temporallyChangingAlpha(stimSeq,rewardRulePattern,choiceSeq,param[0:len(trialGroup)],param[len(trialGroup)],trialGroup)
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue
def costfunc2minimize_modelBased_temporallyChangingAlpha(param,stimSeq,rewardRulePattern,choiceSeq,trialGroup):
  targetValue, AIC, BIC = logLikelihood_modelBased_temporallyChangingAlpha(stimSeq,rewardRulePattern,choiceSeq,param[0:len(trialGroup)],param[len(trialGroup)],trialGroup)
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue

def costfunc2minimize_modelFree_temporallyChangingAlphaAndBeta(param,stimSeq,rewardRulePattern,choiceSeq,trialGroup):
  targetValue, AIC, BIC = logLikelihood_modelFree_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,choiceSeq,param[0:len(trialGroup)],param[len(trialGroup):(2*len(trialGroup))],trialGroup)
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue
def costfunc2minimize_modelBased_temporallyChangingAlphaAndBeta(param,stimSeq,rewardRulePattern,choiceSeq,trialGroup):
  targetValue, AIC, BIC = logLikelihood_modelBased_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,choiceSeq,param[0:len(trialGroup)],param[len(trialGroup):(2*len(trialGroup))],trialGroup)
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue
def costfunc2minimize_modelFree_adaptiveLR(param,stimSeq,rewardRulePattern,choiceSeq):
  targetValue, AIC, BIC, alpha = logLikelihood_modelFree_adaptiveLR(stimSeq,rewardRulePattern,choiceSeq,param[0],param[1],param[2],param[3])
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue

def costfunc2minimize_modelFree_adaptiveLR2(param,stimSeq,rewardRulePattern,choiceSeq):
  targetValue, AIC, BIC, alpha = logLikelihood_modelFree_adaptiveLR2(stimSeq,rewardRulePattern,choiceSeq,param[0],param[1],param[2])
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue

def costfunc2minimize_modelBased_adaptiveLR(param,stimSeq,rewardRulePattern,choiceSeq):
  targetValue, AIC, BIC, alpha = logLikelihood_modelBased_adaptiveLR(stimSeq,rewardRulePattern,choiceSeq,param[0],param[1],param[2],param[3])
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue

def costfunc2minimize_modelBased_adaptiveLR2(param,stimSeq,rewardRulePattern,choiceSeq):
  targetValue, AIC, BIC, alpha = logLikelihood_modelBased_adaptiveLR2(stimSeq,rewardRulePattern,choiceSeq,param[0],param[1],param[2])
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue

def fit_modelFree(stimSeq,rewardRulePattern,choiceSeq):
  numReps = 20
  currentMin = 100000
  for index_rep in range(numReps):
    params_initial=np.array([0.5*np.random.rand(),5.0*np.random.rand()])
    if index_rep < round(numReps*0.5):
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelFree, params_initial, args=(stimSeq,rewardRulePattern,choiceSeq), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      alpha = res['x'][0]
      beta = res['x'][1]
      print(str(index_rep) +'/' + str(numReps) +' was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC = logLikelihood_modelFree(stimSeq,rewardRulePattern,choiceSeq,alpha,beta)
  return logLikelihood, AIC, BIC, alpha, beta
def fit_modelBased(stimSeq,rewardRulePattern,choiceSeq):
  numReps = 20
  currentMin = 100000
  for index_rep in range(numReps):
    params_initial=np.array([0.05*np.random.rand(),5.0*np.random.rand()])
    if index_rep < round(numReps*0.5):
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelBased, params_initial, args=(stimSeq,rewardRulePattern,choiceSeq), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      alpha = res['x'][0]
      beta = res['x'][1]
      print(str(index_rep) +'/' + str(numReps) +' was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC = logLikelihood_modelBased(stimSeq,rewardRulePattern,choiceSeq,alpha,beta)
  return logLikelihood, AIC, BIC, alpha, beta

def fit_modelFree_asymmetric(stimSeq,rewardRulePattern,choiceSeq):
  numReps = 20
  currentMin = 100000
  for index_rep in range(numReps):
    params_initial=np.array([0.5*np.random.rand(),0.5*np.random.rand(),5.0*np.random.rand()])
    if index_rep < round(numReps*0.5):
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelFree_asymmetric, params_initial, args=(stimSeq,rewardRulePattern,choiceSeq), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      alpha_plus = res['x'][0]
      alpha_minus = res['x'][1]
      beta = res['x'][2]
      print(str(index_rep) +'/' + str(numReps) +' was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC = logLikelihood_modelFree_asymmetric(stimSeq,rewardRulePattern,choiceSeq,alpha_plus,alpha_minus,beta)
  return logLikelihood, AIC, BIC, alpha_plus, alpha_minus, beta
def fit_modelBased_asymmetric(stimSeq,rewardRulePattern,choiceSeq):
  numReps = 20
  currentMin = 100000
  for index_rep in range(numReps):
    params_initial=np.array([0.05*np.random.rand(),0.05*np.random.rand(),5.0*np.random.rand()])
    if index_rep < round(numReps*0.5):
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelBased_asymmetric, params_initial, args=(stimSeq,rewardRulePattern,choiceSeq), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      alpha_plus = res['x'][0]
      alpha_minus = res['x'][1]
      beta = res['x'][2]
      print(str(index_rep) +'/' + str(numReps) +' was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC = logLikelihood_modelBased_asymmetric(stimSeq,rewardRulePattern,choiceSeq,alpha_plus,alpha_minus,beta)
  return logLikelihood, AIC, BIC, alpha_plus, alpha_minus, beta

def fit_modelFree_temporallyChangingAlpha(stimSeq,rewardRulePattern,choiceSeq,trialGroup):
  logLikelihood, AIC, BIC, alpha, beta=fit_modelFree(stimSeq,rewardRulePattern,choiceSeq)
  params_initial=np.ones(len(trialGroup)+1)*alpha
  params_initial[len(trialGroup)]=beta

  numReps = 2
  currentMin = 100000
  for index_rep in range(numReps):
    if index_rep ==0:
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelFree_temporallyChangingAlpha , params_initial, args=(stimSeq,rewardRulePattern,choiceSeq,trialGroup), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      alpha = res['x'][0:len(trialGroup)]
      beta = res['x'][len(trialGroup)]
  print('Final estimation with multiple alphas was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC = logLikelihood_modelFree_temporallyChangingAlpha(stimSeq,rewardRulePattern,choiceSeq,alpha,beta,trialGroup)
  return logLikelihood, AIC, BIC, alpha, beta

def fit_modelBased_temporallyChangingAlpha(stimSeq,rewardRulePattern,choiceSeq,trialGroup):
  logLikelihood, AIC, BIC, alpha, beta=fit_modelBased(stimSeq,rewardRulePattern,choiceSeq)
  params_initial=np.ones(len(trialGroup)+1)*alpha
  params_initial[len(trialGroup)]=beta

  numReps = 2
  currentMin = 100000
  for index_rep in range(numReps):
    if index_rep ==0:
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelBased_temporallyChangingAlpha , params_initial, args=(stimSeq,rewardRulePattern,choiceSeq,trialGroup), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      alpha = res['x'][0:len(trialGroup)]
      beta = res['x'][len(trialGroup)]
  print('Final estimation with multiple alphas was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC = logLikelihood_modelBased_temporallyChangingAlpha(stimSeq,rewardRulePattern,choiceSeq,alpha,beta,trialGroup)
  return logLikelihood, AIC, BIC, alpha, beta

#Added on 20210718.
def fit_modelFree_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,choiceSeq,trialGroup):
  logLikelihood, AIC, BIC, alpha, beta=fit_modelFree(stimSeq,rewardRulePattern,choiceSeq)
  params_initial=np.ones(2*len(trialGroup))*alpha
  params_initial[len(trialGroup):(2*len(trialGroup))]=beta

  numReps = 2
  currentMin = 100000
  for index_rep in range(numReps):
    if index_rep ==0:
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelFree_temporallyChangingAlphaAndBeta , params_initial, args=(stimSeq,rewardRulePattern,choiceSeq,trialGroup), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      alpha = res['x'][0:len(trialGroup)]
      beta = res['x'][len(trialGroup):(2*len(trialGroup))]
  print('Final estimation with multiple alphas was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC = logLikelihood_modelFree_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,choiceSeq,alpha,beta,trialGroup)
  return logLikelihood, AIC, BIC, alpha, beta

def fit_modelBased_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,choiceSeq,trialGroup):
  logLikelihood, AIC, BIC, alpha, beta=fit_modelBased(stimSeq,rewardRulePattern,choiceSeq)
  params_initial=np.ones(2*len(trialGroup))*alpha
  params_initial[len(trialGroup):(2*len(trialGroup))]=beta

  numReps = 2
  currentMin = 100000
  for index_rep in range(numReps):
    if index_rep ==0:
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelBased_temporallyChangingAlphaAndBeta , params_initial, args=(stimSeq,rewardRulePattern,choiceSeq,trialGroup), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      alpha = res['x'][0:len(trialGroup)]
      beta = res['x'][len(trialGroup):(2*len(trialGroup))]
  print('Final estimation with multiple alphas was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC = logLikelihood_modelBased_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,choiceSeq,alpha,beta,trialGroup)
  return logLikelihood, AIC, BIC, alpha, beta

#Added on 20210718.
def fit_modelFree_adaptiveLR(stimSeq,rewardRulePattern,choiceSeq):
  logLikelihood, AIC, BIC, alpha, beta=fit_modelFree(stimSeq,rewardRulePattern,choiceSeq)
  params_initial=np.array([np.sqrt(alpha),beta,0.0,np.sqrt(alpha)])

  numReps = 2
  currentMin = 100000
  for index_rep in range(numReps):
    if index_rep ==0:
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelFree_adaptiveLR , params_initial, args=(stimSeq,rewardRulePattern,choiceSeq), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      kappa = res['x'][0]
      beta = res['x'][1]
      eta = res['x'][2]
      alpha0 = res['x'][3]
  print('Final estimation with multiple alphas was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC, alpha = logLikelihood_modelFree_adaptiveLR(stimSeq,rewardRulePattern,choiceSeq,kappa,beta,eta,alpha0)
  return logLikelihood, AIC, BIC, alpha, beta, kappa, eta, alpha0

def fit_modelFree_adaptiveLR2(stimSeq,rewardRulePattern,choiceSeq):
  logLikelihood, AIC, BIC, alpha, beta=fit_modelFree(stimSeq,rewardRulePattern,choiceSeq)
  params_initial=np.array([alpha,beta,0.0])

  numReps = 2
  currentMin = 100000
  for index_rep in range(numReps):
    if index_rep ==0:
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelFree_adaptiveLR2 , params_initial, args=(stimSeq,rewardRulePattern,choiceSeq), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      kappa = res['x'][0]
      beta = res['x'][1]
      eta = res['x'][2]
  print('Final estimation with multiple alphas was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC, alpha = logLikelihood_modelFree_adaptiveLR2(stimSeq,rewardRulePattern,choiceSeq,kappa,beta,eta)
  return logLikelihood, AIC, BIC, alpha, beta, kappa, eta

def fit_modelBased_adaptiveLR(stimSeq,rewardRulePattern,choiceSeq):
  logLikelihood, AIC, BIC, alpha, beta=fit_modelBased(stimSeq,rewardRulePattern,choiceSeq)
  params_initial=np.array([np.sqrt(alpha),beta,0.0,np.sqrt(alpha)])

  numReps = 2
  currentMin = 100000
  for index_rep in range(numReps):
    if index_rep ==0:
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelBased_adaptiveLR , params_initial, args=(stimSeq,rewardRulePattern,choiceSeq), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      kappa = res['x'][0]
      beta = res['x'][1]
      eta = res['x'][2]
      alpha0 = res['x'][3]
  print('Final estimation with multiple alphas was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC, alpha = logLikelihood_modelBased_adaptiveLR(stimSeq,rewardRulePattern,choiceSeq,kappa,beta,eta,alpha0)
  return logLikelihood, AIC, BIC, alpha, beta, kappa, eta, alpha0


def fit_modelBased_adaptiveLR2(stimSeq,rewardRulePattern,choiceSeq):
  logLikelihood, AIC, BIC, alpha, beta=fit_modelBased(stimSeq,rewardRulePattern,choiceSeq)
  params_initial=np.array([alpha,beta,0.0])

  numReps = 2
  currentMin = 100000
  for index_rep in range(numReps):
    if index_rep ==0:
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelBased_adaptiveLR2 , params_initial, args=(stimSeq,rewardRulePattern,choiceSeq), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      kappa = res['x'][0]
      beta = res['x'][1]
      eta = res['x'][2]
  print('Final estimation with multiple alphas was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC, alpha = logLikelihood_modelBased_adaptiveLR2(stimSeq,rewardRulePattern,choiceSeq,kappa,beta,eta)
  return logLikelihood, AIC, BIC, alpha, beta, kappa, eta

#kokomade

def calcBICbasedModelProbability(list_BIC):
  #list_BIC is assumed to be a numpy array with N samples x K models.
  N = list_BIC.shape[0]
  K = list_BIC.shape[1]
  dirAlpha0 = np.ones((1,K))
  dirBeta = np.zeros(K)
  dirAlpha = dirAlpha0
  u = np.zeros(())
  for index_VBiteration in range(100):
    u = np.exp(-0.5*list_BIC+np.dot(np.ones((N,1)),scipy.special.psi(dirAlpha))-scipy.special.psi(np.sum(dirAlpha))*np.ones((N,K)))
    for index_n in range(N):
      u[index_n,:] = u[index_n,:]/np.sum(u[index_n,:])
    for index_k in range(K):
      dirBeta[index_k] = np.sum(u[:,index_k])
    dirAlpha[0,:] = dirAlpha0[0,:] + dirBeta
  dirAlpha = np.squeeze(dirAlpha)
  postProb = dirAlpha/np.sum(dirAlpha)
  return postProb, u

def dellist(items, indexes):
  for index in sorted(indexes, reverse=True):
    del items[index]

def SettrialGroupNovelstim(list_trialGroupEdge):
  #list_trialGroupEdge is assumed to be list as [60, 90, 150, 300]
  trialGroup=list()
  edge = 0
  for i in list_trialGroupEdge:
    trialGroup.append(np.array(range(edge,i)))
    edge = i
  trialGroup = np.array(trialGroup)
  return trialGroup

def SettrialGroupFamiliarstim(rewardRulePattern_list, list_trialGroupEdge):
  #list_trialGroupEdge is assumed to be list as [1, 6]
  trialGroup=list()
  earlyphase = list()
  latephase = list(range(len(rewardRulePattern_list)))
  ReversedTrial = list()
  preRule = 'A'
  for i in range(len(rewardRulePattern_list)):
    if rewardRulePattern_list[i] != preRule:
      ReversedTrial.append(i)
    preRule = rewardRulePattern_list[i]

  if len(list_trialGroupEdge) == 2:
    for i in ReversedTrial:
      for j in range(list_trialGroupEdge[1] - list_trialGroupEdge[0] + 1):
        earlyphase.append(i+j+list_trialGroupEdge[0])
    dellist(latephase, earlyphase)
    trialGroup.append(np.array(earlyphase))
    trialGroup.append(np.array(latephase))
  else:
    trialGroup.append(np.array(list(range(len(rewardRulePattern_list)))))
  return trialGroup

def optimalChoiceRate4FamiliarStimuli(stimSeq, rewardRulePattern,choiceSeq):
  stimuli = ['S1','S2','S3','S4','S5']
  optimalOrNot = np.zeros(len(stimSeq))
  for index_trial in range(len(optimalOrNot)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    currentChoice = stimuli.index(choiceSeq[index_trial])
    if rewardRulePattern[index_trial]=='A':
      if currentChoice==np.max([stimLeft,stimRight]):
        optimalOrNot[index_trial] = 1
    elif rewardRulePattern[index_trial]=='B':
      if currentChoice==np.min([stimLeft,stimRight]):
        optimalOrNot[index_trial] = 1
    else:
      print('Error: Reward rule pattern must be A or B.')
      sys.exit(1)
  ReversedTrial = list()
  preRule = 'A'
  for i in range(len(rewardRulePattern)):
    if rewardRulePattern[i] != preRule:
      ReversedTrial.append(i)
    preRule = rewardRulePattern[i]
  rate_tmp = np.zeros((len(ReversedTrial), 20))
  rate = np.zeros(20)
  for i in range(len(ReversedTrial)):
    for j in range(20):
      rate_tmp[i, j] = optimalOrNot[ReversedTrial[i]-10+j]
  rate = np.mean(rate_tmp, axis = 0)
  
  return rate

# %% start simulation

os.chdir('G:\.shortcut-targets-by-id\\18zZAsW7K45ziRwcsVTpGmNlxYEmlMeGM\Oyama-projects\programs\Python\RLModel_Majima')
# read alphDa and beta file
alpha_list_read = pd.read_csv('Data\\Alphas' + '.csv', encoding='ms932', sep=',', header=None)
alpha_list = np.array(alpha_list_read)  

beta_list_read = pd.read_csv('Data\\Betas' + '.csv', encoding='ms932', sep=',', header=None)
beta_list = np.array(beta_list_read) 

MF_Result_list = list()
MB_Result_list = list()

for fileloop in range(100):
  
  stimSeq = list()
  rewardRulePattern = list()
  blockNoOfReversal = 9
  numRepetitions=100

#import of data files
  filenum = fileloop+1
  
  if os.path.isfile('Data\\rewardRulePattern_' + str(filenum) + '.csv') == False:
    break

  f = open('Data\\' + 'rewardRulePattern_' + str(filenum) + '.csv', 'r')
  rewardRulePattern_list = f.readlines()
  for i in range(len(rewardRulePattern_list)):
    rewardRulePattern_list[i] = rewardRulePattern_list[i].replace('\n', '')
  f.close()

  stimSeq_list = pd.read_csv('Data\\stimSeq_' + str(filenum) + '.csv', encoding='ms932', sep=',', header=None)
  stimSeq_list = np.array(stimSeq_list)   

  stimSeq = list(stimSeq_list)
  rewardRulePattern = list(rewardRulePattern_list)
  choiceSeq = list()

  #Here, we set alpha and beta that temporally changes across trial groups.
  alpha_MF = np.array([alpha_list[fileloop,0],alpha_list[fileloop,1]])
  alpha_MB = np.array([alpha_list[fileloop,2],alpha_list[fileloop,3]])
  beta_MF = np.array([beta_list[fileloop,0],beta_list[fileloop,1]])
  beta_MB = np.array([beta_list[fileloop,2],beta_list[fileloop,3]])

  #Specify trial groups.
  trialGroup=list()
  Novelstim = 0
  
  if Novelstim == 1:
    list_trialGroupedge = ([300], [90, 300], [60, 90, 190, 300])
    trialGroup=SettrialGroupNovelstim(list_trialGroupedge[1]) #Use only second list item.

  else:
    list_trialGroupedge = ([len(rewardRulePattern)], [0, 4])
    trialGroup=SettrialGroupFamiliarstim(rewardRulePattern, list_trialGroupedge[1]) #Use only second list item.
    

  #Do simulation
  if Novelstim == 1:
    rateForModelFree_temporallyChangingAlpha = np.zeros((30,1,numRepetitions))
    rateForModelBased_temporallyChangingAlpha = np.zeros((30,1,numRepetitions))
    for index_repetition in range(numRepetitions):
      #1) Do simulation by the model-free RL model with a temporally changing alpha and beta.
      choiceSeq = simulate_modelFree_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,alpha_MF,beta_MF,trialGroup)
      rateForModelFree_temporallyChangingAlpha[:,0,index_repetition] = optimalChoiceRate(stimSeq, rewardRulePattern,choiceSeq)
      #2) Do simulation by the model-based RL model with a temporally changing alpha and beta.
      choiceSeq = simulate_modelBased_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,alpha_MB,beta_MB,trialGroup)
      rateForModelBased_temporallyChangingAlpha[:,0,index_repetition] = optimalChoiceRate(stimSeq, rewardRulePattern,choiceSeq)
    #take the mean of the optimal choice rate across repetitions.
    rateForModelFree_temporallyChangingAlpha =  np.mean(rateForModelFree_temporallyChangingAlpha,axis=2)
    rateForModelBased_temporallyChangingAlpha =  np.mean(rateForModelBased_temporallyChangingAlpha,axis=2)

  else:
    rateForModelFree_temporallyChangingAlpha = np.zeros((20,1,numRepetitions))
    rateForModelBased_temporallyChangingAlpha = np.zeros((20,1,numRepetitions))
    for index_repetition in range(numRepetitions):
      #1) Do simulation by the model-free RL model with a temporally changing alpha and beta.
      choiceSeq = simulate_modelFree_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,alpha_MF,beta_MF,trialGroup)
      rateForModelFree_temporallyChangingAlpha[:,0,index_repetition] = optimalChoiceRate4FamiliarStimuli(stimSeq, rewardRulePattern,choiceSeq)
      #2) Do simulation by the model-based RL model with a temporally changing alpha and beta.
      choiceSeq = simulate_modelBased_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,alpha_MB,beta_MB,trialGroup)
      rateForModelBased_temporallyChangingAlpha[:,0,index_repetition] = optimalChoiceRate4FamiliarStimuli(stimSeq, rewardRulePattern,choiceSeq)
    #take the mean of the optimal choice rate across repetitions.
    rateForModelFree_temporallyChangingAlpha =  np.mean(rateForModelFree_temporallyChangingAlpha,axis=2)
    rateForModelBased_temporallyChangingAlpha =  np.mean(rateForModelBased_temporallyChangingAlpha,axis=2)

  #Plot the optimal choice rates for the four RL models.
  fig, axs = plt.subplots(2, 2, sharex=True, sharey=True)

  for index_alpha in range(rateForModelFree_temporallyChangingAlpha.shape[1]):
    #1) The results for the model-free RL model
    axs[0,0].plot(100*rateForModelFree_temporallyChangingAlpha[:, index_alpha], marker='o', label= '')
    #2) The results for the model-Based RL model
    axs[0,1].plot(100*rateForModelBased_temporallyChangingAlpha[:, index_alpha], marker='o', label= '')
    axs[0,1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

  axs[0,0].set_title('Model-free RL (non-fixed alpha)')
  axs[0,1].set_title('Model-based RL (non-fixed alpha)')
  axs[0,0].set_xlabel("Block number")
  axs[0,1].set_xlabel("Block number")
  axs[0,0].set_ylabel("Optimal choice rate (%)")


  axs[0,0].set_xlim([-1,31])
  axs[0,0].set_ylim([0,100])
  axs[0,1].set_xlim([-1,31])
  axs[0,1].set_ylim([0,100])
  filename = 'Results\summaryFig_' + str(filenum) + '.png'
  plt.savefig(filename)
  ## plt.show()
  
  
  MF_Result_list.append(100*rateForModelFree_temporallyChangingAlpha[:, 0])
  MB_Result_list.append(100*rateForModelBased_temporallyChangingAlpha[:, 0])
  
MF_Resultarray = np.array(MF_Result_list)
MB_Resultarray = np.array(MB_Result_list)
np.savetxt("Results\MF_Result.csv", MF_Resultarray, delimiter =",",fmt ='% s')
np.savetxt("Results\MB_Result.csv", MB_Resultarray, delimiter =",",fmt ='% s')


# %%
