# -*- coding: utf-8 -*-
"""RLmodelForReversalTask_20210110.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rU-Tnzb1pahQxBv5uTcn7K6LJBVW7McX
"""

#On 20201229, BIC and AIC computation were added.
#On 20201231, the Bayesian model comparison based on "Bayesian model selection for group studies. Neuroimage."ã€€ in Neuroimage 2009.
#On 20210108 (& 20210109), RL models with two leaning rate (alpha) parameters were implemented, which may explain the difference between new-leaning and re-learning.
#On 20210110, to avoid local minima in the estimation of alpha and beta, modify the estimation procedure so that it repeats optimization (asipy.optimize.minimize) 50 times with different initial values and adopts the best ones.
# %%
import numpy as np
import itertools
import matplotlib.pyplot as plt
import scipy.optimize
import scipy.special
import random
import sys
import pandas as pd
import os


# %%

#define basic functions used in this code.
def dellist(items, indexes):
  for index in sorted(indexes, reverse=True):
    del items[index]

def extractlist(items, indexes):
  newlist = list()
  for i in indexes:
    newlist.append(items[i])
  return newlist

#define basic functions used in this code.
def softmax(actionValue, beta):
  x = actionValue*beta
  e_x = np.exp(x - np.max(x))
  output = e_x / e_x.sum()
  # to avoid log(0) computation, the minimum value was introduced.
  np.place(output, output < 10**(-200) , 10**(-200))
  return output

def make1blockStimSequence():
  stimuli = ['S1','S2','S3','S4','S5']
  stimSeq = list(itertools.combinations(stimuli, 2))
  random.shuffle(stimSeq)
  return stimSeq

def returnReward(choice, rewardRulePattern):
  reward = 0
  if rewardRulePattern == 'A':
    if choice == 'S1':
      reward = 1
    elif choice == 'S2':
      reward = 2
    elif choice == 'S3':
      reward = 3
    elif choice == 'S4':
      reward = 4
    elif choice == 'S5':
      reward = 5
    else:
      print('Error: The chosen item must be S1, S2, S3, S4, or S5.')
      sys.exit(1)
  elif rewardRulePattern == 'B':
    if choice == 'S1':
      reward = 5
    elif choice == 'S2':
      reward = 4
    elif choice == 'S3':
      reward = 3
    elif choice == 'S4':
      reward = 2
    elif choice == 'S5':
      reward = 1
    else:
      print('Error: The chosen item must be S1, S2, S3, S4, or S5.')
      sys.exit(1) 
  else:
    print('Error: Reward rule pattern must be A or B.')
    sys.exit(1) 
  return reward

def optimalChoiceRate(stimSeq, rewardRulePattern,choiceSeq):
  stimuli = ['S1','S2','S3','S4','S5']
  optimalOrNot = np.zeros(len(stimSeq))
  for index_trial in range(len(optimalOrNot)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    currentChoice = stimuli.index(choiceSeq[index_trial])
    if rewardRulePattern[index_trial]=='A':
      if currentChoice==np.max([stimLeft,stimRight]):
        optimalOrNot[index_trial] = 1
    elif rewardRulePattern[index_trial]=='B':
      if currentChoice==np.min([stimLeft,stimRight]):
        optimalOrNot[index_trial] = 1
    else:
      print('Error: Reward rule pattern must be A or B.')
      sys.exit(1)
  rate = np.zeros(round(len(stimSeq)/10))
  for index_block in range(round(len(stimSeq)/10)):
    rate[index_block] = np.mean(optimalOrNot[(index_block*10):((index_block+1)*10)])
  return rate

def simulate_modelFree(stimSeq,rewardRulePattern,alpha,beta):
  actionValue = np.zeros(5)
  stimuli = ['S1','S2','S3','S4','S5']
  choiceSeq = list()
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = softmax(np.array([actionValue[stimLeft],actionValue[stimRight]]), beta)
    currentChoice = np.random.choice([stimuli[stimLeft], stimuli[stimRight]], size=(1,), p=prob)
    currentChoice = currentChoice[0]
    choiceSeq.append(currentChoice)
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - actionValue[stimuli.index(currentChoice)]
    actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha*rewardPredictionError
  return choiceSeq

#Added on 20210108.
#In this model, two learning rate parameters are used.
def simulate_modelFree_asymmetric(stimSeq,rewardRulePattern,alpha_plus,alpha_minus,beta):
  actionValue = np.zeros(5)
  stimuli = ['S1','S2','S3','S4','S5']
  choiceSeq = list()
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = softmax(np.array([actionValue[stimLeft],actionValue[stimRight]]), beta)
    currentChoice = np.random.choice([stimuli[stimLeft], stimuli[stimRight]], size=(1,), p=prob)
    currentChoice = currentChoice[0]
    choiceSeq.append(currentChoice)
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - actionValue[stimuli.index(currentChoice)]
    if rewardPredictionError > 0:
      actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha_plus*rewardPredictionError
    else:
      actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha_minus*rewardPredictionError
  return choiceSeq

#Added on 20210221.
#In this model, trials are grouped by the user and alpha is estimated for each trial group.
#For example, if you group trials 0-69 trials, 70-89 trials, 90-159 trials, & 159-299 trials into the four groups (four periods).
#Four alpha parameters are estimated, which are cprresponding to 'learning phase', 'stable phase 1', 're-learning phase' and 'stable phase 2'.
def simulate_modelFree_temporallyChangingAlpha(stimSeq,rewardRulePattern,alpha,beta,trialGroup):
  actionValue = np.zeros(5)
  stimuli = ['S1','S2','S3','S4','S5']
  choiceSeq = list()
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = softmax(np.array([actionValue[stimLeft],actionValue[stimRight]]), beta)
    currentChoice = np.random.choice([stimuli[stimLeft], stimuli[stimRight]], size=(1,), p=prob)
    currentChoice = currentChoice[0]
    choiceSeq.append(currentChoice)
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - actionValue[stimuli.index(currentChoice)]
    #choose used alpha depending on the trial group.
    currentTrialGroup = -1
    for index_trialGroup in range(len(trialGroup)):
      if np.any(trialGroup[index_trialGroup]==index_trial):
        if currentTrialGroup == -1:
          currentTrialGroup = index_trialGroup
        else:
          print('Error: the same trial is used in multiple groups you specified.')
          sys.exit(1)
    if currentTrialGroup == -1:
      print('Error: trial # ' + str(index_trial) + ' does not belong to any group you specified.')
      sys.exit(1)
    actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha[currentTrialGroup]*rewardPredictionError
  return choiceSeq

def simulate_modelBased(stimSeq,rewardRulePattern,alpha,beta):
  modelRatio = 0.5
  stimuli = ['S1','S2','S3','S4','S5']
  choiceSeq = list()
  actionValue4patternA = np.array([1.0,2.0,3.0,4.0,5.0]) 
  actionValue4patternB = np.array([5.0,4.0,3.0,2.0,1.0]) 
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = modelRatio*softmax(np.array([actionValue4patternA[stimLeft],actionValue4patternA[stimRight]]), beta)+(1.0-modelRatio)*softmax(np.array([actionValue4patternB[stimLeft],actionValue4patternB[stimRight]]), beta)
    currentChoice = np.random.choice([stimuli[stimLeft], stimuli[stimRight]], size=(1,), p=prob)
    currentChoice = currentChoice[0]
    choiceSeq.append(currentChoice)
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - modelRatio*actionValue4patternA[stimuli.index(currentChoice)] - (1.0-modelRatio)*actionValue4patternB[stimuli.index(currentChoice)]
    modelRatio = modelRatio + alpha*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    if modelRatio > 1.0:
      modelRatio = 1.0
    elif modelRatio < 0.0:
      modelRatio = 0.0
  return choiceSeq

def simulate_modelBased_asymmetric(stimSeq,rewardRulePattern,alpha_plus,alpha_minus,beta):
  modelRatio = 0.5
  stimuli = ['S1','S2','S3','S4','S5']
  choiceSeq = list()
  actionValue4patternA = np.array([1.0,2.0,3.0,4.0,5.0]) 
  actionValue4patternB = np.array([5.0,4.0,3.0,2.0,1.0]) 
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = modelRatio*softmax(np.array([actionValue4patternA[stimLeft],actionValue4patternA[stimRight]]), beta)+(1.0-modelRatio)*softmax(np.array([actionValue4patternB[stimLeft],actionValue4patternB[stimRight]]), beta)
    currentChoice = np.random.choice([stimuli[stimLeft], stimuli[stimRight]], size=(1,), p=prob)
    currentChoice = currentChoice[0]
    choiceSeq.append(currentChoice)
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - modelRatio*actionValue4patternA[stimuli.index(currentChoice)] - (1.0-modelRatio)*actionValue4patternB[stimuli.index(currentChoice)]
    if rewardPredictionError > 0:
      modelRatio = modelRatio + alpha_plus*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    else:
      modelRatio = modelRatio + alpha_minus*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    if modelRatio > 1.0:
      modelRatio = 1.0
    elif modelRatio < 0.0:
      modelRatio = 0.0
  return choiceSeq

#Added on 20210221.
#In this model, trials are grouped by the user and alpha is estimated for each trial group.
#For example, if you group trials 0-69 trials, 70-89 trials, 90-159 trials, & 159-299 trials into the four groups (four periods).
#Four alpha parameters are estimated, which are cprresponding to 'learning phase', 'stable phase 1', 're-learning phase' and 'stable phase 2'.
def simulate_modelBased_temporallyChangingAlpha(stimSeq,rewardRulePattern,alpha,beta,trialGroup):
  modelRatio = 0.5
  stimuli = ['S1','S2','S3','S4','S5']
  choiceSeq = list()
  actionValue4patternA = np.array([1.0,2.0,3.0,4.0,5.0]) 
  actionValue4patternB = np.array([5.0,4.0,3.0,2.0,1.0]) 
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = modelRatio*softmax(np.array([actionValue4patternA[stimLeft],actionValue4patternA[stimRight]]), beta)+(1.0-modelRatio)*softmax(np.array([actionValue4patternB[stimLeft],actionValue4patternB[stimRight]]), beta)
    currentChoice = np.random.choice([stimuli[stimLeft], stimuli[stimRight]], size=(1,), p=prob)
    currentChoice = currentChoice[0]
    choiceSeq.append(currentChoice)
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - modelRatio*actionValue4patternA[stimuli.index(currentChoice)] - (1.0-modelRatio)*actionValue4patternB[stimuli.index(currentChoice)]
    #choose used alpha depending on the trial group.
    currentTrialGroup = -1
    for index_trialGroup in range(len(trialGroup)):
      if np.any(trialGroup[index_trialGroup]==index_trial):
        if currentTrialGroup == -1:
          currentTrialGroup = index_trialGroup
        else:
          print('Error: the same trial is used in multiple groups you specified.')
          sys.exit(1)
    if currentTrialGroup == -1:
      print('Error: trial # ' + str(index_trial) + ' does not belong to any group you specified.')
      sys.exit(1)
    modelRatio = modelRatio + alpha[currentTrialGroup]*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    if modelRatio > 1.0:
      modelRatio = 1.0
    elif modelRatio < 0.0:
      modelRatio = 0.0
  return choiceSeq

def logLikelihood_modelFree(stimSeq,rewardRulePattern,choiceSeq,alpha,beta):
  actionValue = np.zeros(5)
  stimuli = ['S1','S2','S3','S4','S5']
  logLikelihood = 0
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = softmax(np.array([actionValue[stimLeft],actionValue[stimRight]]), beta)
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - actionValue[stimuli.index(currentChoice)]
    actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha*rewardPredictionError
  if (alpha < 0) or (beta < 0):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + 2.0*2.0
  BIC = -2.0*logLikelihood + 2.0*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC

def logLikelihood_modelFree_asymmetric(stimSeq,rewardRulePattern,choiceSeq,alpha_plus,alpha_minus,beta):
  actionValue = np.zeros(5)
  stimuli = ['S1','S2','S3','S4','S5']
  logLikelihood = 0
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = softmax(np.array([actionValue[stimLeft],actionValue[stimRight]]), beta)
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - actionValue[stimuli.index(currentChoice)]
    if rewardPredictionError > 0:
      actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha_plus*rewardPredictionError
    else:
      actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha_minus*rewardPredictionError
  if ((alpha_plus < 0) or (alpha_minus < 0)) or (beta < 0):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + 3.0*2.0
  BIC = -2.0*logLikelihood + 3.0*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC

def logLikelihood_modelFree_temporallyChangingAlpha(stimSeq,rewardRulePattern,choiceSeq,alpha,beta,trialGroup):
  actionValue = np.zeros(5)
  stimuli = ['S1','S2','S3','S4','S5']
  logLikelihood = 0
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = softmax(np.array([actionValue[stimLeft],actionValue[stimRight]]), beta)
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - actionValue[stimuli.index(currentChoice)]
    #choose used alpha depending on the trial group.
    currentTrialGroup = -1
    for index_trialGroup in range(len(trialGroup)):
      if np.any(trialGroup[index_trialGroup]==index_trial):
        if currentTrialGroup == -1:
          currentTrialGroup = index_trialGroup
        else:
          print('Error: the same trial is used in multiple groups you specified.')
          sys.exit(1)
    if currentTrialGroup == -1:
      print('Error: trial # ' + str(index_trial) + ' does not belong to any group you specified.')
      sys.exit(1)
    actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha[currentTrialGroup]*rewardPredictionError
  if any(alpha < 0) or (beta < 0):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + (len(trialGroup)+1)*2.0
  BIC = -2.0*logLikelihood + (len(trialGroup)+1)*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC

#Added on 20210718.
def logLikelihood_modelFree_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,choiceSeq,alpha,beta,trialGroup):
  actionValue = np.zeros(5)
  stimuli = ['S1','S2','S3','S4','S5']
  logLikelihood = 0
  for index_trial in range(len(stimSeq)):
    #choose used alpha and beta depending on the trial group.
    currentTrialGroup = -1
    for index_trialGroup in range(len(trialGroup)):
      if np.any(trialGroup[index_trialGroup]==index_trial):
        if currentTrialGroup == -1:
          currentTrialGroup = index_trialGroup
        else:
          print('Error: the same trial is used in multiple groups you specified.')
          sys.exit(1)
    if currentTrialGroup == -1:
      print('Error: trial # ' + str(index_trial) + ' does not belong to any group you specified.')
      sys.exit(1)
    #Action choice
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = softmax(np.array([actionValue[stimLeft],actionValue[stimRight]]), beta[currentTrialGroup])
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - actionValue[stimuli.index(currentChoice)]
    actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha[currentTrialGroup]*rewardPredictionError
  if any(alpha < 0) or any(beta < 0):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + (2*len(trialGroup))*2.0
  BIC = -2.0*logLikelihood + (2*len(trialGroup))*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC

#Added on 20210718.
def logLikelihood_modelFree_adaptiveLR(stimSeq,rewardRulePattern,choiceSeq,kappa,beta,eta,alpha0):
  actionValue = np.zeros(5)
  stimuli = ['S1','S2','S3','S4','S5']
  logLikelihood = 0
  alpha = np.zeros(len(stimSeq))
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = softmax(np.array([actionValue[stimLeft],actionValue[stimRight]]), beta)
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    #Compute adaptive alpha using the reward prediction error in the last trial
    if index_trial == 0:
      alpha[index_trial]=alpha0*kappa
    else:
      alpha[index_trial]=kappa*(eta*abs(rewardPredictionError)+(1-eta))#(see Li et al., nature neuroscience, 2011))
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - actionValue[stimuli.index(currentChoice)]
    actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha[index_trial]*rewardPredictionError
  if (kappa < 0) or (beta < 0) or (eta < 0) or (eta > 1):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + 4.0*2.0
  BIC = -2.0*logLikelihood + 4.0*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC, alpha

def logLikelihood_modelFree_adaptiveLR2(stimSeq,rewardRulePattern,choiceSeq,kappa,beta,eta):
  #Almost the same as the above, but alpha0=0.
  actionValue = np.zeros(5)
  stimuli = ['S1','S2','S3','S4','S5']
  logLikelihood = 0
  alpha = np.zeros(len(stimSeq))
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = softmax(np.array([actionValue[stimLeft],actionValue[stimRight]]), beta)
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    #Compute adaptive alpha using the reward prediction error in the last trial
    if index_trial == 0:
      alpha[index_trial]=0
    else:
      alpha[index_trial]=kappa*(eta*abs(rewardPredictionError)+(1-eta))#(see Li et al., nature neuroscience, 2011))
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - actionValue[stimuli.index(currentChoice)]
    actionValue[stimuli.index(currentChoice)] = actionValue[stimuli.index(currentChoice)] + alpha[index_trial]*rewardPredictionError
  if (kappa < 0) or (beta < 0) or (eta < 0) or (eta > 1):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + 3.0*2.0
  BIC = -2.0*logLikelihood + 3.0*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC, alpha

def logLikelihood_modelBased(stimSeq,rewardRulePattern,choiceSeq,alpha,beta):
  modelRatio = 0.5
  stimuli = ['S1','S2','S3','S4','S5']
  actionValue4patternA = np.array([1.0,2.0,3.0,4.0,5.0]) 
  actionValue4patternB = np.array([5.0,4.0,3.0,2.0,1.0]) 
  logLikelihood = 0
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = modelRatio*softmax(np.array([actionValue4patternA[stimLeft],actionValue4patternA[stimRight]]), beta)+(1.0-modelRatio)*softmax(np.array([actionValue4patternB[stimLeft],actionValue4patternB[stimRight]]), beta)
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - modelRatio*actionValue4patternA[stimuli.index(currentChoice)] - (1.0-modelRatio)*actionValue4patternB[stimuli.index(currentChoice)]
    modelRatio = modelRatio + alpha*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    if modelRatio > 1.0:
      modelRatio = 1.0
    elif modelRatio < 0.0:
      modelRatio = 0.0
  if (alpha < 0) or (beta < 0):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + 2.0*2.0
  BIC = -2.0*logLikelihood + 2.0*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC

def logLikelihood_modelBased_asymmetric(stimSeq,rewardRulePattern,choiceSeq,alpha_plus,alpha_minus,beta):
  modelRatio = 0.5
  stimuli = ['S1','S2','S3','S4','S5']
  actionValue4patternA = np.array([1.0,2.0,3.0,4.0,5.0]) 
  actionValue4patternB = np.array([5.0,4.0,3.0,2.0,1.0]) 
  logLikelihood = 0
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = modelRatio*softmax(np.array([actionValue4patternA[stimLeft],actionValue4patternA[stimRight]]), beta)+(1.0-modelRatio)*softmax(np.array([actionValue4patternB[stimLeft],actionValue4patternB[stimRight]]), beta)
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - modelRatio*actionValue4patternA[stimuli.index(currentChoice)] - (1.0-modelRatio)*actionValue4patternB[stimuli.index(currentChoice)]
    if rewardPredictionError > 0:
      modelRatio = modelRatio + alpha_plus*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    else:
      modelRatio = modelRatio + alpha_minus*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    if modelRatio > 1.0:
      modelRatio = 1.0
    elif modelRatio < 0.0:
      modelRatio = 0.0
  if ((alpha_plus < 0) or (alpha_minus < 0)) or (beta < 0):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + 3.0*2.0
  BIC = -2.0*logLikelihood + 3.0*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC

def logLikelihood_modelBased_temporallyChangingAlpha(stimSeq,rewardRulePattern,choiceSeq,alpha,beta,trialGroup):
  modelRatio = 0.5
  stimuli = ['S1','S2','S3','S4','S5']
  actionValue4patternA = np.array([1.0,2.0,3.0,4.0,5.0]) 
  actionValue4patternB = np.array([5.0,4.0,3.0,2.0,1.0]) 
  logLikelihood = 0
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = modelRatio*softmax(np.array([actionValue4patternA[stimLeft],actionValue4patternA[stimRight]]), beta)+(1.0-modelRatio)*softmax(np.array([actionValue4patternB[stimLeft],actionValue4patternB[stimRight]]), beta)
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - modelRatio*actionValue4patternA[stimuli.index(currentChoice)] - (1.0-modelRatio)*actionValue4patternB[stimuli.index(currentChoice)]
    #choose used alpha depending on the trial group.
    currentTrialGroup = -1
    for index_trialGroup in range(len(trialGroup)):
      if np.any(trialGroup[index_trialGroup]==index_trial):
        if currentTrialGroup == -1:
          currentTrialGroup = index_trialGroup
        else:
          print('Error: the same trial is used in multiple groups you specified.')
          sys.exit(1)
    if currentTrialGroup == -1:
      print('Error: trial # ' + str(index_trial) + ' does not belong to any group you specified.')
      sys.exit(1)
    modelRatio = modelRatio + alpha[currentTrialGroup]*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    if modelRatio > 1.0:
      modelRatio = 1.0
    elif modelRatio < 0.0:
      modelRatio = 0.0
  if any(alpha < 0) or (beta < 0):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + (len(trialGroup)+1)*2.0
  BIC = -2.0*logLikelihood + (len(trialGroup)+1)*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC

#Added on 20210718.
def logLikelihood_modelBased_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,choiceSeq,alpha,beta,trialGroup):
  modelRatio = 0.5
  stimuli = ['S1','S2','S3','S4','S5']
  actionValue4patternA = np.array([1.0,2.0,3.0,4.0,5.0]) 
  actionValue4patternB = np.array([5.0,4.0,3.0,2.0,1.0]) 
  logLikelihood = 0
  for index_trial in range(len(stimSeq)):
    #choose used alpha and beta depending on the trial group.
    currentTrialGroup = -1
    for index_trialGroup in range(len(trialGroup)):
      if np.any(trialGroup[index_trialGroup]==index_trial):
        if currentTrialGroup == -1:
          currentTrialGroup = index_trialGroup
        else:
          print('Error: the same trial is used in multiple groups you specified.')
          sys.exit(1)
    if currentTrialGroup == -1:
      print('Error: trial # ' + str(index_trial) + ' does not belong to any group you specified.')
      sys.exit(1)
    #Action choice
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = modelRatio*softmax(np.array([actionValue4patternA[stimLeft],actionValue4patternA[stimRight]]), beta[currentTrialGroup])+(1.0-modelRatio)*softmax(np.array([actionValue4patternB[stimLeft],actionValue4patternB[stimRight]]), beta[currentTrialGroup])
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - modelRatio*actionValue4patternA[stimuli.index(currentChoice)] - (1.0-modelRatio)*actionValue4patternB[stimuli.index(currentChoice)]
    modelRatio = modelRatio + alpha[currentTrialGroup]*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    if modelRatio > 1.0:
      modelRatio = 1.0
    elif modelRatio < 0.0:
      modelRatio = 0.0
  if any(alpha < 0) or any(beta < 0):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + (2*len(trialGroup))*2.0
  BIC = -2.0*logLikelihood + (2*len(trialGroup))*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC

#Added on 20210718.
def logLikelihood_modelBased_adaptiveLR(stimSeq,rewardRulePattern,choiceSeq,kappa,beta,eta,alpha0):
  modelRatio = 0.5
  stimuli = ['S1','S2','S3','S4','S5']
  actionValue4patternA = np.array([1.0,2.0,3.0,4.0,5.0]) 
  actionValue4patternB = np.array([5.0,4.0,3.0,2.0,1.0]) 
  logLikelihood = 0
  alpha = np.zeros(len(stimSeq))
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = modelRatio*softmax(np.array([actionValue4patternA[stimLeft],actionValue4patternA[stimRight]]), beta)+(1.0-modelRatio)*softmax(np.array([actionValue4patternB[stimLeft],actionValue4patternB[stimRight]]), beta)
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    #Compute adaptive alpha using the reward prediction error in the last trial
    if index_trial == 0:
      alpha[index_trial]=alpha0*kappa
    else:
      alpha[index_trial]=kappa*(eta*abs(rewardPredictionError)+(1-eta))#(see Li et al., nature neuroscience, 2011))
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - modelRatio*actionValue4patternA[stimuli.index(currentChoice)] - (1.0-modelRatio)*actionValue4patternB[stimuli.index(currentChoice)]
    modelRatio = modelRatio + alpha[index_trial]*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    if modelRatio > 1.0:
      modelRatio = 1.0
    elif modelRatio < 0.0:
      modelRatio = 0.0
  if (kappa < 0) or (beta < 0) or (eta < 0) or (eta > 1):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + 4.0*2.0
  BIC = -2.0*logLikelihood + 4.0*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC, alpha

def logLikelihood_modelBased_adaptiveLR2(stimSeq,rewardRulePattern,choiceSeq,kappa,beta,eta):
  #Almost the same as the above, but alpha0=0.
  modelRatio = 0.5
  stimuli = ['S1','S2','S3','S4','S5']
  actionValue4patternA = np.array([1.0,2.0,3.0,4.0,5.0]) 
  actionValue4patternB = np.array([5.0,4.0,3.0,2.0,1.0]) 
  logLikelihood = 0
  alpha = np.zeros(len(stimSeq))
  for index_trial in range(len(stimSeq)):
    stimLeft = stimuli.index(stimSeq[index_trial][0])
    stimRight = stimuli.index(stimSeq[index_trial][1])
    prob = modelRatio*softmax(np.array([actionValue4patternA[stimLeft],actionValue4patternA[stimRight]]), beta)+(1.0-modelRatio)*softmax(np.array([actionValue4patternB[stimLeft],actionValue4patternB[stimRight]]), beta)
    if stimSeq[index_trial][0]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[0])
    elif stimSeq[index_trial][1]==choiceSeq[index_trial]:
      logLikelihood = logLikelihood + np.log(prob[1])
    else:
      print('Error: choiceSeq[n] must be stimSeq[n][0] or stimSeq[n][1].')
      sys.exit(1)
    #Compute adaptive alpha using the reward prediction error in the last trial
    if index_trial == 0:
      alpha[index_trial]=0
    else:
      alpha[index_trial]=kappa*(eta*abs(rewardPredictionError)+(1-eta))#(see Li et al., nature neuroscience, 2011))
    currentChoice = choiceSeq[index_trial]
    givenReward = returnReward(currentChoice, rewardRulePattern[index_trial])
    rewardPredictionError = givenReward - modelRatio*actionValue4patternA[stimuli.index(currentChoice)] - (1.0-modelRatio)*actionValue4patternB[stimuli.index(currentChoice)]
    modelRatio = modelRatio + alpha[index_trial]*rewardPredictionError*(actionValue4patternA[stimuli.index(currentChoice)]-actionValue4patternB[stimuli.index(currentChoice)])
    if modelRatio > 1.0:
      modelRatio = 1.0
    elif modelRatio < 0.0:
      modelRatio = 0.0
  if (kappa < 0) or (beta < 0) or (eta < 0) or (eta > 1):
    logLikelihood = np.log(0.1)*len(stimSeq)
  AIC = -2.0*logLikelihood + 3.0*2.0
  BIC = -2.0*logLikelihood + 3.0*np.log(len(stimSeq))
  return logLikelihood, AIC, BIC, alpha

def costfunc2minimize_modelFree(param,stimSeq,rewardRulePattern,choiceSeq):
  targetValue, AIC, BIC = logLikelihood_modelFree(stimSeq,rewardRulePattern,choiceSeq,param[0],param[1])
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue
def costfunc2minimize_modelBased(param,stimSeq,rewardRulePattern,choiceSeq):
  targetValue, AIC, BIC = logLikelihood_modelBased(stimSeq,rewardRulePattern,choiceSeq,param[0],param[1])
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue
def costfunc2minimize_modelFree_asymmetric(param,stimSeq,rewardRulePattern,choiceSeq):
  targetValue, AIC, BIC = logLikelihood_modelFree_asymmetric(stimSeq,rewardRulePattern,choiceSeq,param[0],param[1],param[2])
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue
def costfunc2minimize_modelBased_asymmetric(param,stimSeq,rewardRulePattern,choiceSeq):
  targetValue, AIC, BIC = logLikelihood_modelBased_asymmetric(stimSeq,rewardRulePattern,choiceSeq,param[0],param[1],param[2])
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue
def costfunc2minimize_modelFree_temporallyChangingAlpha(param,stimSeq,rewardRulePattern,choiceSeq,trialGroup):
  targetValue, AIC, BIC = logLikelihood_modelFree_temporallyChangingAlpha(stimSeq,rewardRulePattern,choiceSeq,param[0:len(trialGroup)],param[len(trialGroup)],trialGroup)
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue
def costfunc2minimize_modelBased_temporallyChangingAlpha(param,stimSeq,rewardRulePattern,choiceSeq,trialGroup):
  targetValue, AIC, BIC = logLikelihood_modelBased_temporallyChangingAlpha(stimSeq,rewardRulePattern,choiceSeq,param[0:len(trialGroup)],param[len(trialGroup)],trialGroup)
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue

def costfunc2minimize_modelFree_temporallyChangingAlphaAndBeta(param,stimSeq,rewardRulePattern,choiceSeq,trialGroup):
  targetValue, AIC, BIC = logLikelihood_modelFree_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,choiceSeq,param[0:len(trialGroup)],param[len(trialGroup):(2*len(trialGroup))],trialGroup)
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue
def costfunc2minimize_modelBased_temporallyChangingAlphaAndBeta(param,stimSeq,rewardRulePattern,choiceSeq,trialGroup):
  targetValue, AIC, BIC = logLikelihood_modelBased_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,choiceSeq,param[0:len(trialGroup)],param[len(trialGroup):(2*len(trialGroup))],trialGroup)
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue
def costfunc2minimize_modelFree_adaptiveLR(param,stimSeq,rewardRulePattern,choiceSeq):
  targetValue, AIC, BIC, alpha = logLikelihood_modelFree_adaptiveLR(stimSeq,rewardRulePattern,choiceSeq,param[0],param[1],param[2],param[3])
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue

def costfunc2minimize_modelFree_adaptiveLR2(param,stimSeq,rewardRulePattern,choiceSeq):
  targetValue, AIC, BIC, alpha = logLikelihood_modelFree_adaptiveLR2(stimSeq,rewardRulePattern,choiceSeq,param[0],param[1],param[2])
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue

def costfunc2minimize_modelBased_adaptiveLR(param,stimSeq,rewardRulePattern,choiceSeq):
  targetValue, AIC, BIC, alpha = logLikelihood_modelBased_adaptiveLR(stimSeq,rewardRulePattern,choiceSeq,param[0],param[1],param[2],param[3])
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue

def costfunc2minimize_modelBased_adaptiveLR2(param,stimSeq,rewardRulePattern,choiceSeq):
  targetValue, AIC, BIC, alpha = logLikelihood_modelBased_adaptiveLR2(stimSeq,rewardRulePattern,choiceSeq,param[0],param[1],param[2])
  #weak regularization
  targetValue = -targetValue + 1.0*np.sum(param**2)
  return targetValue

def fit_modelFree(stimSeq,rewardRulePattern,choiceSeq):
  numReps = 20
  currentMin = 100000
  for index_rep in range(numReps):
    params_initial=np.array([0.5*np.random.rand(),5.0*np.random.rand()])
    if index_rep < round(numReps*0.5):
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelFree, params_initial, args=(stimSeq,rewardRulePattern,choiceSeq), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      alpha = res['x'][0]
      beta = res['x'][1]
      print(str(index_rep) +'/' + str(numReps) +' was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC = logLikelihood_modelFree(stimSeq,rewardRulePattern,choiceSeq,alpha,beta)
  return logLikelihood, AIC, BIC, alpha, beta
def fit_modelBased(stimSeq,rewardRulePattern,choiceSeq):
  numReps = 20
  currentMin = 100000
  for index_rep in range(numReps):
    params_initial=np.array([0.05*np.random.rand(),5.0*np.random.rand()])
    if index_rep < round(numReps*0.5):
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelBased, params_initial, args=(stimSeq,rewardRulePattern,choiceSeq), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      alpha = res['x'][0]
      beta = res['x'][1]
      print(str(index_rep) +'/' + str(numReps) +' was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC = logLikelihood_modelBased(stimSeq,rewardRulePattern,choiceSeq,alpha,beta)
  return logLikelihood, AIC, BIC, alpha, beta

def fit_modelFree_asymmetric(stimSeq,rewardRulePattern,choiceSeq):
  numReps = 20
  currentMin = 100000
  for index_rep in range(numReps):
    params_initial=np.array([0.5*np.random.rand(),0.5*np.random.rand(),5.0*np.random.rand()])
    if index_rep < round(numReps*0.5):
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelFree_asymmetric, params_initial, args=(stimSeq,rewardRulePattern,choiceSeq), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      alpha_plus = res['x'][0]
      alpha_minus = res['x'][1]
      beta = res['x'][2]
      print(str(index_rep) +'/' + str(numReps) +' was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC = logLikelihood_modelFree_asymmetric(stimSeq,rewardRulePattern,choiceSeq,alpha_plus,alpha_minus,beta)
  return logLikelihood, AIC, BIC, alpha_plus, alpha_minus, beta
def fit_modelBased_asymmetric(stimSeq,rewardRulePattern,choiceSeq):
  numReps = 20
  currentMin = 100000
  for index_rep in range(numReps):
    params_initial=np.array([0.05*np.random.rand(),0.05*np.random.rand(),5.0*np.random.rand()])
    if index_rep < round(numReps*0.5):
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelBased_asymmetric, params_initial, args=(stimSeq,rewardRulePattern,choiceSeq), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      alpha_plus = res['x'][0]
      alpha_minus = res['x'][1]
      beta = res['x'][2]
      print(str(index_rep) +'/' + str(numReps) +' was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC = logLikelihood_modelBased_asymmetric(stimSeq,rewardRulePattern,choiceSeq,alpha_plus,alpha_minus,beta)
  return logLikelihood, AIC, BIC, alpha_plus, alpha_minus, beta

def fit_modelFree_temporallyChangingAlpha(stimSeq,rewardRulePattern,choiceSeq,trialGroup):
  logLikelihood, AIC, BIC, alpha, beta=fit_modelFree(stimSeq,rewardRulePattern,choiceSeq)
  params_initial=np.ones(len(trialGroup)+1)*alpha
  params_initial[len(trialGroup)]=beta

  numReps = 2
  currentMin = 100000
  for index_rep in range(numReps):
    if index_rep ==0:
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelFree_temporallyChangingAlpha , params_initial, args=(stimSeq,rewardRulePattern,choiceSeq,trialGroup), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      alpha = res['x'][0:len(trialGroup)]
      beta = res['x'][len(trialGroup)]
  print('Final estimation with multiple alphas was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC = logLikelihood_modelFree_temporallyChangingAlpha(stimSeq,rewardRulePattern,choiceSeq,alpha,beta,trialGroup)
  return logLikelihood, AIC, BIC, alpha, beta

def fit_modelBased_temporallyChangingAlpha(stimSeq,rewardRulePattern,choiceSeq,trialGroup):
  logLikelihood, AIC, BIC, alpha, beta=fit_modelBased(stimSeq,rewardRulePattern,choiceSeq)
  params_initial=np.ones(len(trialGroup)+1)*alpha
  params_initial[len(trialGroup)]=beta

  numReps = 2
  currentMin = 100000
  for index_rep in range(numReps):
    if index_rep ==0:
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelBased_temporallyChangingAlpha , params_initial, args=(stimSeq,rewardRulePattern,choiceSeq,trialGroup), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      alpha = res['x'][0:len(trialGroup)]
      beta = res['x'][len(trialGroup)]
  print('Final estimation with multiple alphas was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC = logLikelihood_modelBased_temporallyChangingAlpha(stimSeq,rewardRulePattern,choiceSeq,alpha,beta,trialGroup)
  return logLikelihood, AIC, BIC, alpha, beta

#Added on 20210718.
def fit_modelFree_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,choiceSeq,trialGroup):
  logLikelihood, AIC, BIC, alpha, beta=fit_modelFree(stimSeq,rewardRulePattern,choiceSeq)
  params_initial=np.ones(2*len(trialGroup))*alpha
  params_initial[len(trialGroup):(2*len(trialGroup))]=beta

  numReps = 2
  currentMin = 100000
  for index_rep in range(numReps):
    if index_rep ==0:
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelFree_temporallyChangingAlphaAndBeta , params_initial, args=(stimSeq,rewardRulePattern,choiceSeq,trialGroup), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      alpha = res['x'][0:len(trialGroup)]
      beta = res['x'][len(trialGroup):(2*len(trialGroup))]
  print('Final estimation with multiple alphas was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC = logLikelihood_modelFree_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,choiceSeq,alpha,beta,trialGroup)
  return logLikelihood, AIC, BIC, alpha, beta

def fit_modelBased_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,choiceSeq,trialGroup):
  logLikelihood, AIC, BIC, alpha, beta=fit_modelBased(stimSeq,rewardRulePattern,choiceSeq)
  params_initial=np.ones(2*len(trialGroup))*alpha
  params_initial[len(trialGroup):(2*len(trialGroup))]=beta

  numReps = 2
  currentMin = 100000
  for index_rep in range(numReps):
    if index_rep ==0:
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelBased_temporallyChangingAlphaAndBeta , params_initial, args=(stimSeq,rewardRulePattern,choiceSeq,trialGroup), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      alpha = res['x'][0:len(trialGroup)]
      beta = res['x'][len(trialGroup):(2*len(trialGroup))]
  print('Final estimation with multiple alphas was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC = logLikelihood_modelBased_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,choiceSeq,alpha,beta,trialGroup)
  return logLikelihood, AIC, BIC, alpha, beta

#Added on 20210718.#kokomade
def fit_modelFree_adaptiveLR(stimSeq,rewardRulePattern,choiceSeq):
  logLikelihood, AIC, BIC, alpha, beta=fit_modelFree(stimSeq,rewardRulePattern,choiceSeq)
  params_initial=np.array([np.sqrt(alpha),beta,0.0,np.sqrt(alpha)])

  numReps = 2
  currentMin = 100000
  for index_rep in range(numReps):
    if index_rep ==0:
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelFree_adaptiveLR , params_initial, args=(stimSeq,rewardRulePattern,choiceSeq), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      kappa = res['x'][0]
      beta = res['x'][1]
      eta = res['x'][2]
      alpha0 = res['x'][3]
  print('Final estimation with multiple alphas was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC, alpha = logLikelihood_modelFree_adaptiveLR(stimSeq,rewardRulePattern,choiceSeq,kappa,beta,eta,alpha0)
  return logLikelihood, AIC, BIC, alpha, beta, kappa, eta, alpha0

def fit_modelFree_adaptiveLR2(stimSeq,rewardRulePattern,choiceSeq):
  logLikelihood, AIC, BIC, alpha, beta=fit_modelFree(stimSeq,rewardRulePattern,choiceSeq)
  params_initial=np.array([alpha,beta,0.0])

  numReps = 2
  currentMin = 100000
  for index_rep in range(numReps):
    if index_rep ==0:
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelFree_adaptiveLR2 , params_initial, args=(stimSeq,rewardRulePattern,choiceSeq), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      kappa = res['x'][0]
      beta = res['x'][1]
      eta = res['x'][2]
  print('Final estimation with multiple alphas was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC, alpha = logLikelihood_modelFree_adaptiveLR2(stimSeq,rewardRulePattern,choiceSeq,kappa,beta,eta)
  return logLikelihood, AIC, BIC, alpha, beta, kappa, eta

def fit_modelBased_adaptiveLR(stimSeq,rewardRulePattern,choiceSeq):
  logLikelihood, AIC, BIC, alpha, beta=fit_modelBased(stimSeq,rewardRulePattern,choiceSeq)
  params_initial=np.array([np.sqrt(alpha),beta,0.0,np.sqrt(alpha)])

  numReps = 2
  currentMin = 100000
  for index_rep in range(numReps):
    if index_rep ==0:
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelBased_adaptiveLR , params_initial, args=(stimSeq,rewardRulePattern,choiceSeq), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      kappa = res['x'][0]
      beta = res['x'][1]
      eta = res['x'][2]
      alpha0 = res['x'][3]
  print('Final estimation with multiple alphas was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC, alpha = logLikelihood_modelBased_adaptiveLR(stimSeq,rewardRulePattern,choiceSeq,kappa,beta,eta,alpha0)
  return logLikelihood, AIC, BIC, alpha, beta, kappa, eta, alpha0


def fit_modelBased_adaptiveLR2(stimSeq,rewardRulePattern,choiceSeq):
  logLikelihood, AIC, BIC, alpha, beta=fit_modelBased(stimSeq,rewardRulePattern,choiceSeq)
  params_initial=np.array([alpha,beta,0.0])

  numReps = 2
  currentMin = 100000
  for index_rep in range(numReps):
    if index_rep ==0:
      optMethod = 'Nelder-Mead'
    else:
      optMethod = 'Powell'
    res = scipy.optimize.minimize(costfunc2minimize_modelBased_adaptiveLR2 , params_initial, args=(stimSeq,rewardRulePattern,choiceSeq), method=optMethod, tol=1e-10)
    if currentMin > res['fun']:
      currentMin = res['fun']
      kappa = res['x'][0]
      beta = res['x'][1]
      eta = res['x'][2]
  print('Final estimation with multiple alphas was done, currentMin:' + str(currentMin))
  logLikelihood, AIC, BIC, alpha = logLikelihood_modelBased_adaptiveLR2(stimSeq,rewardRulePattern,choiceSeq,kappa,beta,eta)
  return logLikelihood, AIC, BIC, alpha, beta, kappa, eta

#kokomade

def calcBICbasedModelProbability(list_BIC):
  #list_BIC is assumed to be a numpy array with N samples x K models.
  N = list_BIC.shape[0]
  K = list_BIC.shape[1]
  dirAlpha0 = np.ones((1,K))
  dirBeta = np.zeros(K)
  dirAlpha = dirAlpha0
  u = np.zeros(())
  for index_VBiteration in range(100):
    u = np.exp(-0.5*list_BIC+np.dot(np.ones((N,1)),scipy.special.psi(dirAlpha))-scipy.special.psi(np.sum(dirAlpha))*np.ones((N,K)))
    for index_n in range(N):
      u[index_n,:] = u[index_n,:]/np.sum(u[index_n,:])
    for index_k in range(K):
      dirBeta[index_k] = np.sum(u[:,index_k])
    dirAlpha[0,:] = dirAlpha0[0,:] + dirBeta
  dirAlpha = np.squeeze(dirAlpha)
  postProb = dirAlpha/np.sum(dirAlpha)
  return postProb, u

def SettrialGroupNovelstim(list_trialGroupEdge):
  #list_trialGroupEdge is assumed to be list as [60, 90, 150, 300]
  trialGroup=list()
  edge = 0
  for i in list_trialGroupEdge:
    trialGroup.append(np.array(range(edge,i)))
    edge = i
  trialGroup = np.array(trialGroup)
  return trialGroup

def SettrialGroupFamiliarstim(rewardRulePattern_list, list_trialGroupEdge):
  #list_trialGroupEdge is assumed to be list as [1, 6]
  trialGroup=list()
  earlyphase = list()
  latephase = list(range(len(rewardRulePattern_list)))
  ReversedTrial = list()
  preRule = 'A'
  for i in range(len(rewardRulePattern_list)):
    if rewardRulePattern_list[i] != preRule:
      ReversedTrial.append(i)
    preRule = rewardRulePattern_list[i]

  if len(list_trialGroupEdge) == 2:
    for i in ReversedTrial:
      for j in range(list_trialGroupEdge[1] - list_trialGroupEdge[0] + 1):
        earlyphase.append(i+j+list_trialGroupEdge[0])
    dellist(latephase, earlyphase)
    trialGroup.append(np.array(earlyphase))
    trialGroup.append(np.array(latephase))
  else:
    trialGroup.append(np.array(list(range(len(rewardRulePattern_list)))))

  return trialGroup

def SqueezeSession(choiceSeq_list, rewardRulePattern_list, stimSeq_list):
  preRule = 'A'
  ReversedTrial = list()
  Trial_s = list()
  choiceSeq_list_s = list()
  rewardRulePattern_list_s = list()
  stimSeq_list_s = list()

  for i in range(len(choiceSeq_list)):
    if rewardRulePattern_list[i] != preRule:
      ReversedTrial.append(i)
    preRule = rewardRulePattern_list[i]
  
  for i in ReversedTrial:
    if len(choiceSeq_list) > i + 20:
      for k in range(i-10, i+20):
        Trial_s.append(k)
  choiceSeq_list_s = extractlist(choiceSeq_list, Trial_s)
  rewardRulePattern_list_s = extractlist(rewardRulePattern_list, Trial_s)
  stimSeq_list_s = extractlist(stimSeq_list, Trial_s)

  return choiceSeq_list_s, rewardRulePattern_list_s, stimSeq_list_s
# %%

BICs = list()
Alphas = list()
Betas = list()


# %%
os.chdir('G:\.shortcut-targets-by-id\\18zZAsW7K45ziRwcsVTpGmNlxYEmlMeGM\Oyama-projects\programs\Python\RLModel_Majima')
for fileloop in range(100):
  #import of data files
  filenum = fileloop+1
  
  if os.path.isfile('Data\\choiceSeq_' + str(filenum) + '.csv') == False:
    break

  f = open('Data\\choiceSeq_' + str(filenum) + '.csv', 'r')
  choiceSeq_list = f.readlines()
  for i in range(len(choiceSeq_list)):
    choiceSeq_list[i] = choiceSeq_list[i].replace('\n', '')
  f.close()

  f = open('Data\\' + 'rewardRulePattern_' + str(filenum) + '.csv', 'r')
  rewardRulePattern_list = f.readlines()
  for i in range(len(rewardRulePattern_list)):
    rewardRulePattern_list[i] = rewardRulePattern_list[i].replace('\n', '')
  f.close()

  stimSeq_list = pd.read_csv('Data\\stimSeq_' + str(filenum) + '.csv', encoding='ms932', sep=',', header=None)
  stimSeq_list = np.array(stimSeq_list)
  f.close()    

  BICs_temp = list()
  alpha_temp = list()
  beta_temp = list()

  #BIC comparison using a given simulation data
  stimSeq = list(stimSeq_list)
  rewardRulePattern = list(rewardRulePattern_list)
  choiceSeq = list(choiceSeq_list)

  stimSeq_s = list()
  rewardRulePattern_s = list()
  choiceSeq_s = list()
  blockNoOfReversal = 9
  beta = 1.0

  Novelstim = 0
  list_trialGroupedge = ([len(rewardRulePattern)], [0, 4])
  # %%
  #Create simulation data. We choose one model here.
  #choiceSeq = simulate_modelFree(stimSeq,rewardRulePattern,0.05,beta)
  #choiceSeq = simulate_modelBased(stimSeq,rewardRulePattern,0.005,beta)
  #choiceSeq = simulate_modelFree_asymmetric(stimSeq,rewardRulePattern,0.05,0.0,beta)
  #choiceSeq = simulate_modelBased_asymmetric(stimSeq,rewardRulePattern,0.005,0.0,beta)

  #Compute BIC using each of the four models

  f = open('Results\summary_' + str(filenum) + '.txt', mode='w')

  print('estimation starts')
  #BIC = np.zeros((1,4))
  BIC = np.zeros((1,2)) 

  if Novelstim == 1:
    trialGroup=SettrialGroupNovelstim(list_trialGroupedge[1])
  else:
    trialGroup=SettrialGroupFamiliarstim(rewardRulePattern_list, list_trialGroupedge[1])
  logLikelihood, AIC, BIC[0,0], alpha, beta = fit_modelFree_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,choiceSeq,trialGroup)
  print('Model 3 (model-free, different alpha and beta for different trial groups)')
  print('alpha: ' + str(alpha))
  print('beta: ' + str(beta))
  print('BIC: ' + str(BIC[0,0]))
  print(' ')
  f.write('Model 3 (model-free, different alpha and beta for different trial groups)\n')
  f.write('alpha: ' + str(alpha) + '\n')
  f.write('beta: ' + str(beta) + '\n')
  f.write('BIC: ' + str(BIC[0,0]) + '\n')
  f.write('')
  for i in alpha:
    alpha_temp.append(i)
  for i in beta:
    beta_temp.append(i)
  BICs_temp.append(BIC[0,0])

  if Novelstim == 1:
    trialGroup=SettrialGroupNovelstim(list_trialGroupedge[1])
  else:
    trialGroup=SettrialGroupFamiliarstim(rewardRulePattern_list, list_trialGroupedge[1])
  logLikelihood, AIC, BIC[0,1], alpha, beta = fit_modelBased_temporallyChangingAlphaAndBeta(stimSeq,rewardRulePattern,choiceSeq,trialGroup)
  print('Model 4 (model-based, different alpha and beta for different trial groups)')
  print('alpha: ' + str(alpha))
  print('beta: ' + str(beta))
  print('BIC: ' + str(BIC[0,1]))
  print(' ')
  f.write('Model 4 (model-based, different alpha and beta for different trial groups)\n')
  f.write('alpha: ' + str(alpha) + '\n')
  f.write('beta: ' + str(beta) + '\n')
  f.write('BIC: ' + str(BIC[0,1]) + '\n')
  f.write('')
  for i in alpha:
    alpha_temp.append(i)
  for i in beta:
    beta_temp.append(i)
  BICs_temp.append(BIC[0,1])

  f.close()
  BICs.append(BICs_temp)
  Alphas.append(alpha_temp)
  Betas.append(beta_temp)

  #Calculate the BIC-based posterior model probability
  BICbasedModelProbability, u = calcBICbasedModelProbability(BIC)

  #Calculate the optimal choice rates for the individual blocks.
  rate = optimalChoiceRate(stimSeq, rewardRulePattern,choiceSeq)

  #Plot the results.

  fig, axs = plt.subplots(1, 2, figsize=(9, 3), sharey=True)
  axs[0].plot(100*rate, marker='o')
  axs[0].set_xlabel("Block number")
  axs[0].set_ylabel("Optimal choice rate (%)")
  axs[0].set_xlim([-1,31])
  axs[0].set_ylim([0,100])

  names = ['ModelF_2','ModelB_2']
  # names = ['Model 1','Model 2']
  axs[1].bar(names, 100*(BICbasedModelProbability))
  axs[1].set_ylabel("BIC-based model probability (%)")
  axs[1].set_xlabel("RL model")
  axs[1].set_ylim([0,100])

  # save fig
  filename = 'Results\summaryFig_' + str(filenum) + '.png'
  plt.savefig(filename)

#%%
np.savetxt("Results\BICs.csv", BICs, delimiter =",",fmt ='% s')
np.savetxt("Results\Alphas.csv", Alphas, delimiter =",",fmt ='% s')
np.savetxt("Results\Betas.csv", Betas, delimiter =",",fmt ='% s')
BICarray = np.array(BICs)
postProb, u = calcBICbasedModelProbability(BICarray)
np.savetxt("Results\postProb.csv", postProb, delimiter =",",fmt ='% s')

# %%
  # plt.show()
  # %%
